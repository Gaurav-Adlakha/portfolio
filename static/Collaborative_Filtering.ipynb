{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: Collaborative Filtering From Scratch\n",
    "author: \"Gaurav Adlakha\"\n",
    "date: \"2025-05-15\"\n",
    "categories: [fastai,Deep-Learning,Machine-Learning,Recommender-Systems]\n",
    "description: \"Collaborative filtering system from scratch using fastai and PyTorch\"\n",
    "toc: true\n",
    "toc-depth: 3\n",
    "format:\n",
    "  html:\n",
    "    code-fold: true\n",
    "    code-tools: true\n",
    "    highlight-style: github\n",
    "    fig-width: 8\n",
    "    fig-height: 6\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaborative Filtering from Scratch\n",
    "\n",
    ">Have you ever wondered how Netflix recommends movies you might like, or how Amazon suggests products you might want to buy? Behind these recommendations is often a technique called *collaborative filtering*. In this post, we'll build a collaborative filtering system from scratch using PyTorch and fastai.\n",
    "\n",
    "## What is Collaborative Filtering?\n",
    "\n",
    "Collaborative filtering is a technique that makes predictions about what a user might like based on the preferences of many other users. The basic idea is simple: if person A likes items 1, 2, and 3, and person B likes items 1, 2, and 4, then person A might also like item 4.\n",
    "\n",
    "Let's dive right in and start building our own collaborative filtering system using the MovieLens dataset.\n",
    "\n",
    "## Setting Up\n",
    "\n",
    "First, let's import the necessary libraries:\n",
    "\n",
    "```python\n",
    "from fastai.imports import *\n",
    "from fastai.collab import *\n",
    "from fastai.tabular.all import *\n",
    "set_seed(42)\n",
    "```\n",
    "\n",
    "The fastai library provides high-level components that make it easier to build deep learning models. We're using modules from `collab` and `tabular` for our collaborative filtering task, and setting a random seed for reproducibility.\n",
    "\n",
    "## Getting the Data\n",
    "\n",
    "We'll use the MovieLens 100k dataset, which contains 100,000 movie ratings from 943 users on 1,682 movies:\n",
    "\n",
    "```python\n",
    "path = untar_data(URLs.ML_100k)\n",
    "path.ls()\n",
    "```\n",
    "\n",
    "```\n",
    "(#23) [Path('/Users/gaurav.adlakha/.fastai/data/ml-100k/u.item'),Path('/Users/gaurav.adlakha/.fastai/data/ml-100k/u3.test'),Path('/Users/gaurav.adlakha/.fastai/data/ml-100k/u1.base'),Path('/Users/gaurav.adlakha/.fastai/data/ml-\n",
    "```\n",
    "\n",
    "The `untar_data` function downloads and extracts the dataset for us. We can see that there are many files in this dataset, including training and test splits.\n",
    "\n",
    "Let's load the main ratings data:\n",
    "\n",
    "```python\n",
    "ratings = pd.read_csv(path/'u.data', delimiter='\\t', header=None,\n",
    "                  names=['user','movie','rating','timestamp'])\n",
    "ratings.head()\n",
    "```\n",
    "\n",
    "|    | user | movie | rating | timestamp |\n",
    "|----|------|-------|--------|-----------|\n",
    "| 0  | 196  | 242   | 3      | 881250949 |\n",
    "| 1  | 186  | 302   | 3      | 891717742 |\n",
    "| 2  | 22   | 377   | 1      | 878887116 |\n",
    "| 3  | 244  | 51    | 2      | 880606923 |\n",
    "| 4  | 166  | 346   | 1      | 886397596 |\n",
    "\n",
    "Perfect! We now have a dataframe with four columns:\n",
    "- `user`: The ID of the user who gave the rating\n",
    "- `movie`: The ID of the movie being rated\n",
    "- `rating`: The rating given (1-5 stars)\n",
    "- `timestamp`: When the rating was given\n",
    "\n",
    "This is the foundation we need to start building our collaborative filtering model. In the next part, we'll explore the data further and start building our model.\n",
    "\n",
    "## Understanding the Data\n",
    "\n",
    "From our initial look, we can see that users are identified by numbers (like 196, 186, 22) and movies are also identified by numbers (242, 302, 377). The ratings range from 1 to 5, with 1 being the lowest and 5 being the highest.\n",
    "\n",
    "The core idea behind collaborative filtering is to find patterns in these ratings. For example, if user 196 and user 186 both gave high ratings to similar movies, they might have similar tastes. This means if user 196 likes a movie that user 186 hasn't seen yet, we might recommend that movie to user 186.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the Ratings Matrix\n",
    "\n",
    "To get a better understanding of our data, let's create a cross-tabulation that shows how our most active users rated the most popular movies:\n",
    "\n",
    "```python\n",
    "def get_top_ratings(df, n_users=20, n_movies=20):\n",
    "    \"Return crosstab of ratings from top users for top movies\"\n",
    "    top_users = df.user.value_counts().index[:n_users]\n",
    "    top_movies = df.movie.value_counts().index[:n_movies]\n",
    "    filtered = df[(df.user.isin(top_users)) & (df.movie.isin(top_movies))]\n",
    "    return pd.crosstab(filtered.user, filtered.movie, filtered.rating, aggfunc='mean')\n",
    "\n",
    "ratings_matrix = get_top_ratings(ratings)\n",
    "```\n",
    "\n",
    "This function selects the top 20 users who have rated the most movies and the top 20 movies that have received the most ratings. It then creates a matrix where rows represent users, columns represent movies, and each cell contains the rating that a user gave to a movie."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movie Ratings Table\n",
    "\n",
    "| User/Movie | 1 | 7 | 50 | 56 | 98 | 100 | 117 | 121 | 127 | 172 | 174 | 181 | 204 | 222 |\n",
    "|------------|---|---|----|----|----|----|-----|-----|-----|-----|-----|-----|-----|-----|\n",
    "| 7 | N/A | 5.0 | 5.0 | 5.0 | 4.0 | 5.0 | N/A | 5.0 | 5.0 | 4.0 | 5.0 | 3.0 | 5.0 | N/A |\n",
    "| 13 | 3.0 | 2.0 | 5.0 | 5.0 | 4.0 | 5.0 | 3.0 | 5.0 | 5.0 | 5.0 | 4.0 | 5.0 | 5.0 | 3.0 |\n",
    "| 92 | 4.0 | 4.0 | 5.0 | 5.0 | 5.0 | 5.0 | 4.0 | 5.0 | N/A | 4.0 | 5.0 | 4.0 | 4.0 | 4.0 |\n",
    "| 94 | 4.0 | 4.0 | 5.0 | 5.0 | 4.0 | 5.0 | N/A | 2.0 | 5.0 | 4.0 | 4.0 | 4.0 | 4.0 | 3.0 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This matrix gives us a visual representation of our data. Each row is a user, each column is a movie, and the values are the ratings (1-5). The `NaN` values indicate that a user hasn't rated that particular movie.\n",
    "\n",
    "Looking at this matrix, we can observe several patterns:\n",
    "\n",
    "1. Some users tend to give higher ratings overall (like user 276 and user 416), while others are more critical.\n",
    "2. Some movies are generally rated higher than others.\n",
    "3. There are many missing values (`NaN`), which is typical in recommendation systems - users only rate a small fraction of all available items.\n",
    "\n",
    "This sparsity is one of the key challenges in building recommendation systems. We need to predict the missing values based on the patterns in the existing ratings.\n",
    "\n",
    "## The Dot Product: A Measure of Similarity\n",
    "\n",
    "At the heart of collaborative filtering is the idea of representing users and items as vectors in a shared space. The similarity between a user and an item can then be calculated using the dot product of their vectors.\n",
    "\n",
    "Let's see a simple example:\n",
    "\n",
    "```python\n",
    "movie = np.array([0.98, 0.9, -0.9])\n",
    "user = np.array([0.9, 0.8, -0.6])\n",
    "(user*movie).sum()\n",
    "```\n",
    "\n",
    "```\n",
    "np.float64(2.1420000000000003)\n",
    "```\n",
    "\n",
    "In this example, we have:\n",
    "- A movie vector `[0.98, 0.9, -0.9]`\n",
    "- A user vector `[0.9, 0.8, -0.6]`\n",
    "\n",
    "The dot product is calculated by multiplying corresponding elements and then summing them:\n",
    "(0.98 × 0.9) + (0.9 × 0.8) + (-0.9 × -0.6) = 0.882 + 0.72 + 0.54 = 2.142\n",
    "\n",
    "This value (2.142) can be interpreted as a measure of how well this movie matches this user's preferences. The higher the dot product, the better the match.\n",
    "\n",
    "In a real recommendation system, these vectors would have more dimensions (often called \"latent factors\" or \"embeddings\"), and they would be learned from the data rather than manually specified. Each dimension might represent some aspect of movies or user preferences, such as genre, pace, tone, etc., though these dimensions are usually not interpretable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When Users Don't Like Movies\n",
    "\n",
    "We just saw that a high dot product between user and movie vectors can indicate a good match. But what happens when a user doesn't like a movie? Let's see:\n",
    "\n",
    "```python\n",
    "movie = np.array([0.98,0.9,-0.9])\n",
    "user = np.array([0.1,-1.0,-0.6])\n",
    "(user*movie).sum()\n",
    "```\n",
    "\n",
    "```\n",
    "np.float64(-0.262)\n",
    "```\n",
    "\n",
    "In this case, we get a negative value (-0.262). This makes intuitive sense: when a user's preferences are opposite to a movie's characteristics, the dot product becomes negative, indicating a poor match.\n",
    "\n",
    "Let's break down what's happening:\n",
    "- (0.98 × 0.1) = 0.098 (small positive contribution)\n",
    "- (0.9 × -1.0) = -0.9 (large negative contribution)\n",
    "- (-0.9 × -0.6) = 0.54 (positive contribution from matching negative features)\n",
    "\n",
    "The overall result is negative, suggesting this user would probably not enjoy this movie.\n",
    "\n",
    "This simple example shows how embeddings can capture the essence of user preferences and movie characteristics in a way that allows us to predict how well they match.\n",
    "\n",
    "## Adding Movie Titles\n",
    "\n",
    "So far, we've been working with movie IDs, which aren't very informative. Let's load the movie titles so we can better understand our data:\n",
    "\n",
    "```python\n",
    "movies = pd.read_csv(path/'u.item', delimiter='|', header=None, encoding='latin1', \n",
    "                    usecols=[0,1], names=['movie','title'])\n",
    "movies.head()\n",
    "```\n",
    "\n",
    "|   | movie | title            |\n",
    "|---|-------|------------------|\n",
    "| 0 | 1     | Toy Story (1995) |\n",
    "| 1 | 2     | GoldenEye (1995) |\n",
    "| 2 | 3     | Four Rooms (1995)|\n",
    "| 3 | 4     | Get Shorty (1995)|\n",
    "| 4 | 5     | Copycat (1995)   |\n",
    "\n",
    "Now we can see that movie ID 1 corresponds to \"Toy Story (1995)\", movie ID 2 is \"GoldenEye (1995)\", and so on. This will be helpful when we start making predictions and recommendations.\n",
    "\n",
    "Let's also look at our full ratings dataset again to remind ourselves what we're working with:\n",
    "\n",
    "```python\n",
    "ratings\n",
    "```\n",
    "\n",
    "```\n",
    "       user  movie  rating  timestamp\n",
    "0       196    242       3  881250949\n",
    "1       186    302       3  891717742\n",
    "2        22    377       1  878887116\n",
    "3       244     51       2  880606923\n",
    "4       166    346       1  886397596\n",
    "...     ...    ...     ...        ...\n",
    "99995   880    476       3  880175444\n",
    "99996   716    204       5  879795543\n",
    "99997   276   1090       1  874795795\n",
    "99998    13    225       2  882399156\n",
    "99999    12    203       3  879959583\n",
    "\n",
    "[100000 rows x 4 columns]\n",
    "```\n",
    "\n",
    "We have 100,000 ratings from 943 users on 1,682 movies. This is a substantial dataset that should allow us to build a reasonably good recommendation system.\n",
    "\n",
    "## The Core Idea: Learning Embeddings\n",
    "\n",
    "Now that we understand the data and have seen how dot products can measure similarity, let's discuss the core idea behind our collaborative filtering model.\n",
    "\n",
    "The goal is to learn an embedding vector for each user and each movie. These embeddings will be positioned in a shared space so that when a user likes a movie, their vectors are close together (resulting in a high dot product), and when a user dislikes a movie, their vectors are far apart (resulting in a low dot product).\n",
    "\n",
    "For example, if a user enjoys action movies with fast-paced plots, their embedding might have high values in dimensions representing \"action\" and \"fast-paced\". Movies with similar characteristics would also have high values in these dimensions, leading to a high dot product and thus a high predicted rating.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining Ratings with Movie Titles\n",
    "\n",
    "Now let's merge our ratings data with the movie titles to make our dataset more interpretable:\n",
    "\n",
    "```python\n",
    "ratings= ratings.merge(movies)\n",
    "ratings\n",
    "```\n",
    "\n",
    "Perfect! Now our ratings dataframe includes the movie titles alongside the IDs. This makes it much easier to understand what movies users are rating. For example, we can see that user 196 gave \"Kolya (1996)\" a rating of 3, and user 716 gave \"Back to the Future (1985)\" a perfect 5.\n",
    "\n",
    "## Setting Up for Model Training\n",
    "\n",
    "Now, let's create a fastai `CollabDataLoaders` object, which will handle the data preparation for our collaborative filtering model:\n",
    "\n",
    "```python\n",
    "dls= CollabDataLoaders.from_df(ratings, item_name='title',bs=64)\n",
    "dls.show_batch()\n",
    "```\n",
    "\n",
    "\n",
    "The `CollabDataLoaders` class takes care of several important preprocessing steps:\n",
    "1. Splitting the data into training and validation sets\n",
    "2. Converting user IDs and movie titles into categorical variables\n",
    "3. Creating mini-batches for efficient training\n",
    "4. Handling the sparse nature of the data\n",
    "\n",
    "Once we have our DataLoader set up, we can examine some basic information about our dataset:\n",
    "\n",
    "```python\n",
    "n_users= len(dls.classes['user'])\n",
    "n_movies = len(dls.classes['title'])\n",
    "n_movies\n",
    "```\n",
    "\n",
    "This would tell us how many unique users and movies we have in our dataset. When the code runs successfully, we'd see that we have 943 users and 1,665 movies with ratings.\n",
    "\n",
    "The fastai `CollabDataLoaders` provides a convenient abstraction over the data preparation process. It automatically handles the conversion of our raw dataframe into a format suitable for training collaborative filtering models, saving us from having to write a lot of boilerplate code.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the Dataset Dimensions\n",
    "\n",
    "Let's first check how many unique users we have in our dataset:\n",
    "\n",
    "```python\n",
    "n_users\n",
    "```\n",
    "\n",
    "```\n",
    "944\n",
    "```\n",
    "\n",
    "We have 944 unique users in our dataset. This number will be important as we build our embedding matrices.\n",
    "\n",
    "## Creating Embedding Factors\n",
    "\n",
    "Now, let's manually create embedding matrices for users and movies. These are randomly initialized tensors that will later be trained to capture patterns in our data:\n",
    "\n",
    "```python\n",
    "user_factor = torch.randn(n_users,5)\n",
    "movie_factor= torch.randn(n_movies,5)\n",
    "```\n",
    "\n",
    "We've created two tensors:\n",
    "1. `user_factor`: A tensor of shape (944, 5) containing random values\n",
    "2. `movie_factor`: A tensor of shape (1665, 5) containing random values\n",
    "\n",
    "The number 5 here represents the embedding dimension - we're representing each user and movie as a 5-dimensional vector. This is a hyperparameter that we can adjust; larger values allow for more complex representations but require more data to train effectively.\n",
    "\n",
    "Let's check the shapes of these tensors to confirm:\n",
    "\n",
    "```python\n",
    "user_factor.shape\n",
    "```\n",
    "\n",
    "```\n",
    "torch.Size([944, 5])\n",
    "```\n",
    "\n",
    "And for the movie factors:\n",
    "\n",
    "```python\n",
    "movie_factor.shape\n",
    "```\n",
    "\n",
    "```\n",
    "torch.Size([1665, 5])\n",
    "```\n",
    "\n",
    "These shapes confirm that we have 944 users and 1,665 movies, each represented by a 5-dimensional embedding vector.\n",
    "\n",
    "## Accessing Individual Embeddings\n",
    "\n",
    "We can access the embedding for a specific user or movie by indexing into these tensors. For example, to get the embedding for user with ID 5:\n",
    "\n",
    "```python\n",
    "torch.embedding(user_factor,torch.tensor(5))\n",
    "```\n",
    "\n",
    "```\n",
    "tensor([-1.2018, -1.2946, -1.8869,  1.2259,  0.2970])\n",
    "```\n",
    "\n",
    "This shows the 5-dimensional embedding vector for user 5. These values are currently random, but after training, they will capture meaningful patterns about this user's preferences.\n",
    "\n",
    "## The Embedding Concept\n",
    "\n",
    "These embedding vectors are at the heart of collaborative filtering. The idea is that after training:\n",
    "\n",
    "1. Users with similar tastes will have similar embedding vectors\n",
    "2. Movies with similar characteristics will have similar embedding vectors\n",
    "3. The dot product between a user's vector and a movie's vector will predict how much that user would like that movie\n",
    "\n",
    "For example, if two users both enjoy sci-fi movies, their embedding vectors might have high values in similar dimensions. Similarly, if two movies are both action films, their embedding vectors might align in certain dimensions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using PyTorch's Embedding Layer\n",
    "\n",
    "Rather than working with raw tensors, PyTorch provides a specialized `nn.Embedding` layer that's designed for exactly this use case. Let's create embedding layers for our users:\n",
    "\n",
    "```python\n",
    "user_emb= nn.Embedding(n_users,5)\n",
    "```\n",
    "\n",
    "This creates an embedding layer for our users with 5 dimensions. The advantage of using `nn.Embedding` over raw tensors is that it integrates with PyTorch's automatic differentiation, making it easy to train these embeddings using gradient descent.\n",
    "\n",
    "Let's look at the embedding for user 10:\n",
    "\n",
    "```python\n",
    "user_emb.weight[10]\n",
    "```\n",
    "\n",
    "```\n",
    "tensor([-1.0398, -1.7286, -0.6798,  2.5864, -0.1015],\n",
    "       grad_fn=<SelectBackward0>)\n",
    "```\n",
    "\n",
    "This shows the initial random values for user 10's embedding. Note the `grad_fn` attribute, which indicates that this tensor is part of PyTorch's computation graph and can be updated during training.\n",
    "\n",
    "For comparison, let's look at our original manually created embedding tensor:\n",
    "\n",
    "```python\n",
    "user_factor\n",
    "```\n",
    "\n",
    "```\n",
    "tensor([[-1.0827,  0.2138,  0.9310, -0.2739, -0.4359],\n",
    "        [-0.5195,  0.7613, -0.4365,  0.1365,  1.3300],\n",
    "        [-1.2804,  0.0705,  0.6489, -1.2110,  1.8266],\n",
    "        ...,\n",
    "        [ 0.8009, -0.4734, -0.8962, -0.7348, -0.0246],\n",
    "        [ 0.3354, -0.8262, -0.1541,  0.4699,  0.4873],\n",
    "        [ 2.4054, -0.2156, -1.4126, -0.2467,  1.0571]])\n",
    "```\n",
    "\n",
    "And specifically for user 10:\n",
    "\n",
    "```python\n",
    "user_factor[10]\n",
    "```\n",
    "\n",
    "```\n",
    "tensor([-0.5753,  0.1556, -0.3694,  0.4986, -2.5438])\n",
    "```\n",
    "\n",
    "The values are different because each was randomly initialized.\n",
    "\n",
    "Finally, let's inspect the embedding layer itself:\n",
    "\n",
    "```python\n",
    "user_emb\n",
    "```\n",
    "\n",
    "```\n",
    "Embedding(944, 5)\n",
    "```\n",
    "\n",
    "This confirms that our embedding layer is set up correctly with 944 users and 5 dimensions per user.\n",
    "\n",
    "## Understanding PyTorch Embeddings\n",
    "\n",
    "The `nn.Embedding` layer is essentially a lookup table. When we pass an index (like a user ID), it returns the corresponding row from its weight matrix. For example, when we access `user_emb.weight[10]`, we're getting the 11th row (since indexing starts at 0) of the embedding weight matrix.\n",
    "\n",
    "During training, these embedding weights will be updated to minimize our loss function. Users who rate similar movies similarly will end up with similar embedding vectors.\n",
    "\n",
    "In the next section, we'll create a complete neural network model that uses these embeddings to predict ratings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Embeddings as Matrix Operations\n",
    "\n",
    "One important insight about embeddings is that they're equivalent to a specific matrix operation:\n",
    "\n",
    "Taking the dot product with a one-hot encoding of a vector is the same as looking up that vector at a particular index.\n",
    "\n",
    "This helps us understand what's happening \"under the hood\" with embedding layers. Let's explore this with some examples:\n",
    "\n",
    "```python\n",
    "one_hot(3,100)\n",
    "```\n",
    "\n",
    "```\n",
    "tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "        0, 0, 0, 0], dtype=torch.uint8)\n",
    "```\n",
    "\n",
    "The `one_hot` function creates a vector with all zeros except for a single 1 at the specified index. Here, we've created a one-hot vector of length 100 with a 1 at index 3.\n",
    "\n",
    "Let's convert it to float type for matrix operations:\n",
    "\n",
    "```python\n",
    "one_hot(3,100).float()\n",
    "```\n",
    "\n",
    "```\n",
    "tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
    "```\n",
    "\n",
    "Now, if we had a matrix and multiplied it by this one-hot vector, it would effectively select the 4th row of the matrix (index 3, since we start counting from 0).\n",
    "\n",
    "This is exactly what an embedding lookup does. When we access:\n",
    "\n",
    "```python\n",
    "user_emb.weight[10]\n",
    "```\n",
    "\n",
    "```\n",
    "tensor([-1.0398, -1.7286, -0.6798,  2.5864, -0.1015],\n",
    "       grad_fn=<SelectBackward0>)\n",
    "```\n",
    "\n",
    "It's equivalent to multiplying the embedding weight matrix by a one-hot vector with a 1 at index 10.\n",
    "\n",
    "Let's look at our embedding layer again:\n",
    "\n",
    "```python\n",
    "user_emb\n",
    "```\n",
    "\n",
    "```\n",
    "Embedding(944, 5)\n",
    "```\n",
    "\n",
    "This shows we have an embedding matrix of shape (944, 5), where each row corresponds to a user and each column represents a dimension of our embedding space.\n",
    "\n",
    "## Why This Matters\n",
    "\n",
    "Understanding embeddings as matrix operations helps us grasp what's happening during the forward and backward passes of training:\n",
    "\n",
    "1. **Forward pass**: When we look up an embedding, we're selecting a specific row from the embedding matrix.\n",
    "\n",
    "2. **Backward pass**: When we compute gradients, we're only updating the specific rows that were used in the forward pass, not the entire matrix.\n",
    "\n",
    "This is why embeddings are so efficient for representing categorical variables with many possible values. We only need to compute and update the embeddings for the specific categories present in each batch.\n",
    "\n",
    "In the next section, we'll build a complete collaborative filtering model that uses these embedding layers to predict user ratings for movies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Exploring Embeddings in More Detail\n",
    "\n",
    "Let's explore the embedding concept more thoroughly by examining the shapes and operations involved:\n",
    "\n",
    "```python\n",
    "user_emb.weight.shape\n",
    "```\n",
    "\n",
    "```\n",
    "torch.Size([944, 5])\n",
    "```\n",
    "\n",
    "This confirms that our embedding matrix has 944 rows (one per user) and 5 columns (the embedding dimension).\n",
    "\n",
    "Now let's look at the shape of a one-hot encoded vector for user 10:\n",
    "\n",
    "```python\n",
    "one_hot(10,n_users).float().shape\n",
    "```\n",
    "\n",
    "```\n",
    "torch.Size([944])\n",
    "```\n",
    "\n",
    "This is a vector with 944 elements (one per user), with all zeros except for a 1 at index 10.\n",
    "\n",
    "When we multiply the transpose of our embedding matrix by this one-hot vector, we get the embedding for user 10:\n",
    "\n",
    "```python\n",
    "user_emb.weight.t() @ one_hot(10,n_users).float()\n",
    "```\n",
    "\n",
    "```\n",
    "tensor([-1.0398, -1.7286, -0.6798,  2.5864, -0.1015], grad_fn=<MvBackward0>)\n",
    "```\n",
    "\n",
    "This is exactly the same as what we get when we directly access `user_emb.weight[10]`. This demonstrates that an embedding lookup is mathematically equivalent to multiplying by a one-hot vector.\n",
    "\n",
    "## What is an Embedding?\n",
    "\n",
    "An embedding is a learned mapping from discrete objects (like users or movies) to vectors of continuous numbers. In the context of collaborative filtering:\n",
    "\n",
    "1. Each user and movie is represented by a vector of floating-point numbers\n",
    "2. These vectors are learned during training to optimize a specific objective (like predicting ratings)\n",
    "3. The embedding vectors capture latent features that aren't explicitly provided in the data\n",
    "\n",
    "Let's create another embedding layer to see how PyTorch initializes them:\n",
    "\n",
    "```python\n",
    "u_e= Embedding(944, 5)\n",
    "```\n",
    "\n",
    "And look at the embedding for user 10:\n",
    "\n",
    "```python\n",
    "u_e.weight[10]\n",
    "```\n",
    "\n",
    "```\n",
    "tensor([0.0063, 0.0057, 0.0053, 0.0052, 0.0039], grad_fn=<SelectBackward0>)\n",
    "```\n",
    "\n",
    "Notice that this is different from our previous embedding for user 10. PyTorch initializes embeddings randomly, so each time we create a new embedding layer, we get different starting values.\n",
    "Finally, let's get a batch of data to see what our model will receive during training:\n",
    "\n",
    "```python\n",
    "batch = dls.one_batch()\n",
    "```\n",
    "\n",
    "This gives us a single batch from our dataloader, which contains:\n",
    "1. Input data: user IDs and movie IDs\n",
    "2. Target data: the corresponding ratings\n",
    "\n",
    "## Understanding the Batch Structure\n",
    "\n",
    "When we retrieve a batch from our dataloader, it contains two main components:\n",
    "\n",
    "1. **Inputs**: A tensor of shape (batch_size, 2) where each row contains a user ID and a movie ID\n",
    "2. **Targets**: A tensor of shape (batch_size,) containing the rating that each user gave to the corresponding movie\n",
    "\n",
    "During training, our model will:\n",
    "1. Look up the embeddings for each user and movie in the batch\n",
    "2. Compute the dot product of each user-movie pair\n",
    "3. Compare the predicted ratings to the actual ratings\n",
    "4. Update the embeddings to minimize the prediction error\n",
    "\n",
    "This is the essence of the collaborative filtering approach: learning embeddings that capture user preferences and movie characteristics in a way that allows us to predict how users will rate movies they haven't seen yet.\n",
    "\n",
    "In the next section, we'll build a complete neural network model that implements this approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Building Our Collaborative Filtering Model\n",
    "\n",
    "Now that we understand embeddings, let's put everything together to build a complete collaborative filtering model. First, let's verify the dimensions of our user and movie spaces:\n",
    "\n",
    "```python\n",
    "944,1665\n",
    "```\n",
    "\n",
    "```\n",
    "(944, 1665)\n",
    "```\n",
    "\n",
    "This confirms we have 944 users and 1,665 movies in our dataset.\n",
    "\n",
    "When we get a batch of data, it contains user and movie IDs. Let's examine the movie IDs in a batch:\n",
    "\n",
    "```python\n",
    "batch[0][:,1]\n",
    "```\n",
    "\n",
    "```\n",
    "tensor([1330,  899,  230, 1391,  334, 1133,  897,  466,  668,  102,  236, 1443,\n",
    "         528,  320, 1247,  256,  769,  143,  271, 1397,  210, 1544, 1442,  529,\n",
    "          17,  611, 1052,  485,  623, 1525,  938,  503, 1544,   65,  816, 1227,\n",
    "          93,  499,  179, 1179,  588, 1019,  304,    5,  710,  457,  861, 1006,\n",
    "         320,  578,  899,   62,  177,  279,  328, 1496,  570, 1252, 1216, 1402,\n",
    "         884,  457,  738, 1121])\n",
    "```\n",
    "\n",
    "These are the movie IDs in our batch. Each will be used to look up the corresponding movie embedding.\n",
    "\n",
    "Let's create embeddings for users and movies and see what shape we get when we look up a batch:\n",
    "\n",
    "```python\n",
    "us_em=Embedding(944,5)\n",
    "mo_em=Embedding(1665,5)\n",
    "\n",
    "batch = dls.one_batch()\n",
    "\n",
    "x= batch[0][:,0]\n",
    "y= batch[0][:,1]\n",
    "\n",
    "print(us_em(x).shape, mo_em(y).shape)\n",
    "```\n",
    "\n",
    "```\n",
    "torch.Size([64, 5]) torch.Size([64, 5])\n",
    "```\n",
    "\n",
    "This shows that for a batch of 64 examples, we get 64 user embeddings and 64 movie embeddings, each with 5 dimensions.\n",
    "\n",
    "## Implementing the Collaborative Filtering Model\n",
    "\n",
    "Now let's implement our collaborative filtering model:\n",
    "\n",
    "```python\n",
    "class CollabNN(nn.Module):\n",
    "    \"Simple collaborative filtering model with embeddings\"\n",
    "    def __init__(self, n_users, n_items, n_factors=5):\n",
    "        super().__init__()\n",
    "        self.user_factors = nn.Embedding(n_users, n_factors)\n",
    "        self.item_factors = nn.Embedding(n_items, n_factors)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        users, items = x[:,0], x[:,1]\n",
    "        u_embs = self.user_factors(users)\n",
    "        i_embs = self.item_factors(items)\n",
    "        return (u_embs * i_embs).sum(dim=1)\n",
    "```\n",
    "\n",
    "This model:\n",
    "1. Creates embedding layers for users and items\n",
    "2. In the forward pass, extracts user and item IDs from the input\n",
    "3. Looks up the corresponding embeddings\n",
    "4. Computes the element-wise product of user and item embeddings\n",
    "5. Sums along the embedding dimension to get a single prediction per user-item pair\n",
    "\n",
    "Let's create an instance of our model and get a batch of data:\n",
    "\n",
    "```python\n",
    "model = CollabNN(n_users, n_movies)\n",
    "batch = dls.one_batch()\n",
    "```\n",
    "\n",
    "Now let's run the model on a batch to see what it predicts:\n",
    "\n",
    "```python\n",
    "model(batch[0])\n",
    "```\n",
    "\n",
    "```\n",
    "tensor([ 7.0029,  2.1804,  0.4648, -0.3704, -3.6570, -2.5748, -1.2354, -0.4618,\n",
    "         0.8670, -0.7257, -1.3684,  0.6179,  0.6468,  2.9299,  1.4910, -0.4101,\n",
    "         0.6326, -0.6567,  2.2093,  2.4966, -5.0130, -3.6183,  3.9021,  4.4451,\n",
    "        -0.0432, -2.0280, -3.6852, -5.9757, -1.5701, -1.1312,  0.8875, -1.5192,\n",
    "        -1.2604, -0.9187, -1.3469, -0.6555, -1.2011, -0.6149,  0.3042,  1.4095,\n",
    "        -1.7217,  0.3008, -0.0148,  0.3080,  2.2792,  3.7195, -0.1592, -0.6061,\n",
    "         1.7568, -0.7674,  0.2440, -0.9074, -1.0106, -3.1345,  0.0641,  1.2300,\n",
    "         3.4579, -0.4415, -1.4399, -3.0345,  1.6182, -1.2363, -1.8696, -1.8537],\n",
    "       grad_fn=<SumBackward1>)\n",
    "```\n",
    "\n",
    "Let's check the shape of these predictions:\n",
    "\n",
    "```python\n",
    "model(batch[0]).shape\n",
    "```\n",
    "\n",
    "```\n",
    "torch.Size([64])\n",
    "```\n",
    "\n",
    "We get 64 predictions, one for each user-item pair in our batch. These are the predicted ratings.\n",
    "\n",
    "## Training the Model\n",
    "\n",
    "Now let's create a learner to train our model:\n",
    "\n",
    "```python\n",
    "mdl =CollabNN(n_users, n_movies)\n",
    "learner = Learner(dls, mdl, loss_func=MSELossFlat())\n",
    "```\n",
    "\n",
    "The `Learner` class from fastai takes care of the training loop, optimization, and other details. We use Mean Squared Error as our loss function, which is appropriate for regression tasks like rating prediction.\n",
    "\n",
    "Let's train the model for 5 epochs using the one-cycle policy:\n",
    "\n",
    "```python\n",
    "learner.fit_one_cycle(5, 5e-3)\n",
    "```\n",
    "\n",
    "```\n",
    "epoch     train_loss  valid_loss  time    \n",
    "0         16.511957   16.503958   00:05     \n",
    "1         12.656537   13.187530   00:05     \n",
    "2         4.729345    5.090328    00:06     \n",
    "3         2.651412    3.003635    00:05     \n",
    "4         2.324853    2.789970    00:05     \n",
    "```\n",
    "\n",
    "The training and validation losses decrease over time, indicating that our model is learning to predict ratings. By the end of training, we have a Mean Squared Error of around 2.3 on the training set and 2.8 on the validation set.\n",
    "\n",
    "Finally, let's get a batch of data to use for making predictions:\n",
    "\n",
    "```python\n",
    "x,y = dls.one_batch()\n",
    "```\n",
    "\n",
    "## What We've Built\n",
    "\n",
    "We've successfully built and trained a collaborative filtering model that can predict how users will rate movies they haven't seen yet. The model learns embeddings for both users and movies, capturing latent features that determine user preferences and movie characteristics.\n",
    "\n",
    "The key steps were:\n",
    "1. Creating embedding layers for users and movies\n",
    "2. Building a neural network that computes the dot product of user and movie embeddings\n",
    "3. Training the model to minimize the prediction error on known ratings\n",
    "\n",
    "This model can now be used to recommend movies to users by predicting ratings for movies they haven't seen and suggesting those with the highest predicted ratings.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Evaluating and Improving Our Model\n",
    "\n",
    "Now that we've trained our basic collaborative filtering model, let's examine how well it's performing and explore ways to improve it. \n",
    "\n",
    "The model we built has a simple structure: it computes the dot product between user and movie embeddings to predict ratings. While this works reasonably well, there are several improvements we can make.\n",
    "\n",
    "### Adding Bias Terms\n",
    "\n",
    "One limitation of our current model is that it doesn't account for user and movie biases. Some users tend to give higher ratings overall, and some movies tend to receive higher ratings regardless of who's rating them.\n",
    "\n",
    "Let's create an improved model that includes bias terms:\n",
    "\n",
    "```python\n",
    "class CollabNN(nn.Module):\n",
    "    \"Collaborative filtering model with embeddings and bias terms\"\n",
    "    def __init__(self, n_users, n_items, n_factors=5, y_range=(0,5.5)):\n",
    "        super().__init__()\n",
    "        self.user_factors = nn.Embedding(n_users, n_factors)\n",
    "        self.item_factors = nn.Embedding(n_items, n_factors)\n",
    "        self.user_bias = nn.Embedding(n_users, 1)\n",
    "        self.item_bias = nn.Embedding(n_items, 1)\n",
    "        self.y_range = y_range\n",
    "        \n",
    "    def forward(self, x):\n",
    "        users, items = x[:,0], x[:,1]\n",
    "        u_embs = self.user_factors(users)\n",
    "        i_embs = self.item_factors(items)\n",
    "        u_bias = self.user_bias(users).squeeze()\n",
    "        i_bias = self.item_bias(items).squeeze()\n",
    "        dot = (u_embs * i_embs).sum(dim=1)\n",
    "        return sigmoid_range((dot + u_bias + i_bias), *self.y_range)\n",
    "```\n",
    "\n",
    "This model adds several improvements:\n",
    "\n",
    "1. **User and Item Biases**: Each user and movie has a bias term that captures their general rating tendencies.\n",
    "\n",
    "2. **Output Range**: We use `sigmoid_range` to constrain the output to a specific range (0 to 5.5 in this case). This ensures our predictions are within the valid rating range.\n",
    "\n",
    "Let's train this improved model:\n",
    "\n",
    "```python\n",
    "mdl = CollabNN(n_users, n_movies)\n",
    "learner = Learner(dls, mdl, loss_func=MSELossFlat())\n",
    "```\n",
    "\n",
    "```python\n",
    "learner.fit_one_cycle(5, 5e-3)\n",
    "```\n",
    "\n",
    "```\n",
    "epoch     train_loss  valid_loss  time    \n",
    "0         3.944084    3.918710    00:06     \n",
    "1         1.731161    1.895458    00:06     \n",
    "2         1.111672    1.324167    00:06     \n",
    "3         1.025903    1.186654    00:06     \n",
    "4         0.962344    1.169547    00:06 \n",
    "```\n",
    "\n",
    "The results are much better! Our validation loss has decreased from 2.79 to 1.17, which is a substantial improvement. This shows that adding bias terms and constraining the output range has helped our model make more accurate predictions.\n",
    "\n",
    "Let's visualize the training progress:\n",
    "\n",
    "```python\n",
    "def get_training_losses(learner):\n",
    "    \"Display training and validation losses from a fastai Learner as a DataFrame.\"\n",
    "    losses = learner.recorder.values\n",
    "    train_losses = [x[0] for x in losses]\n",
    "    valid_losses = [x[1] for x in losses]\n",
    "    \n",
    "    return pd.DataFrame({\n",
    "        'Epoch': range(1, len(train_losses)+1),\n",
    "        'Training Loss': train_losses,\n",
    "        'Validation Loss': valid_losses\n",
    "    })\n",
    "```\n",
    "\n",
    "```python\n",
    "get_training_losses(learner)\n",
    "```\n",
    "\n",
    "|   | Epoch | Training Loss | Validation Loss |\n",
    "|---|-------|---------------|----------------|\n",
    "| 0 | 1     | 3.944084      | 3.918710       |\n",
    "| 1 | 2     | 1.731161      | 1.895458       |\n",
    "| 2 | 3     | 1.111672      | 1.324167       |\n",
    "| 3 | 4     | 1.025903      | 1.186654       |\n",
    "| 4 | 5     | 0.962344      | 1.169547       |\n",
    "\n",
    "The loss decreases rapidly in the first few epochs and then continues to improve more gradually. This is a typical learning curve for neural networks.\n",
    "\n",
    "### Understanding the Model's Predictions\n",
    "\n",
    "Our model now predicts ratings in the range of 0 to 5.5. The Mean Squared Error of about 1.17 on the validation set indicates that, on average, our predictions are off by about 1.08 stars (the square root of the MSE).\n",
    "\n",
    "This is quite good for a simple collaborative filtering model! It means that if a user would rate a movie 4 stars, our model might predict 3 or 5 stars, which is reasonable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Enhancing Our Model\n",
    "\n",
    "Looking at the remaining cells in the notebook, we can see that there's one more enhancement we can make to our collaborative filtering model: adding a global bias term.\n",
    "\n",
    "```python\n",
    "class ModifiedCollabNN(nn.Module):\n",
    "    \"Collaborative filtering model with embeddings and bias terms\"\n",
    "    def __init__(self, n_users, n_items, n_factors=5, y_range=(0,5.5)):\n",
    "        super().__init__()\n",
    "        self.user_factors = nn.Embedding(n_users, n_factors)\n",
    "        self.item_factors = nn.Embedding(n_items, n_factors)\n",
    "        self.user_bias = nn.Embedding(n_users, 1)\n",
    "        self.item_bias = nn.Embedding(n_items, 1)\n",
    "        self.bias = nn.Parameter(torch.zeros(1))\n",
    "        self.y_range = y_range\n",
    "        \n",
    "    def forward(self, x):\n",
    "        users, items = x[:,0], x[:,1]\n",
    "        u_embs = self.user_factors(users)\n",
    "        i_embs = self.item_factors(items)\n",
    "        u_bias = self.user_bias(users).squeeze()\n",
    "        i_bias = self.item_bias(items).squeeze()\n",
    "        dot = (u_embs * i_embs).sum(dim=1)\n",
    "        return sigmoid_range(dot + u_bias + i_bias + self.bias, *self.y_range)\n",
    "```\n",
    "\n",
    "This model adds a global bias term (`self.bias`) that captures the overall average rating in the dataset. This gives us a complete model with:\n",
    "\n",
    "1. User embeddings\n",
    "2. Movie embeddings\n",
    "3. User bias terms\n",
    "4. Movie bias terms\n",
    "5. Global bias term\n",
    "6. Output range constraint\n",
    "\n",
    "Let's train this enhanced model:\n",
    "\n",
    "```python\n",
    "mdl = ModifiedCollabNN(n_users, n_movies)\n",
    "learner = Learner(dls, mdl, loss_func=MSELossFlat())\n",
    "```\n",
    "\n",
    "```python\n",
    "learner.fit_one_cycle(5, 5e-3, wd=0.1)\n",
    "```\n",
    "\n",
    "The training results show that the model converges to a validation loss of around 0.87, which is even better than our previous model. The weight decay parameter (`wd=0.1`) helps prevent overfitting by regularizing the model parameters.\n",
    "\n",
    "Let's visualize the training progress:\n",
    "\n",
    "```python\n",
    "losses = learner.recorder.values\n",
    "\n",
    "# Get training and validation losses (first two columns of values)\n",
    "train_losses = [x[0] for x in losses]\n",
    "valid_losses = [x[1] for x in losses]\n",
    "\n",
    "# Create a dataframe to display them\n",
    "pd.DataFrame({\n",
    "    'Epoch': range(1, len(train_losses)+1),\n",
    "    'Training Loss': train_losses,\n",
    "    'Validation Loss': valid_losses\n",
    "})\n",
    "```\n",
    "\n",
    "|   | Epoch | Training Loss | Validation Loss |\n",
    "|---|-------|---------------|----------------|\n",
    "| 0 | 1     | 0.872213      | 1.016872       |\n",
    "| 1 | 2     | 0.806509      | 0.916550       |\n",
    "| 2 | 3     | 0.790299      | 0.886949       |\n",
    "| 3 | 4     | 0.777781      | 0.872037       |\n",
    "| 4 | 5     | 0.759944      | 0.869526       |\n",
    "\n",
    "## Comparing the Models\n",
    "\n",
    "Let's compare the three models we've built:\n",
    "\n",
    "1. **Basic Dot Product Model**:\n",
    "   - Simple dot product between user and movie embeddings\n",
    "   - Final validation loss: ~2.79\n",
    "\n",
    "2. **Model with Bias Terms and Output Range**:\n",
    "   - User and movie embeddings\n",
    "   - User and movie bias terms\n",
    "   - Output range constraint\n",
    "   - Final validation loss: ~1.17\n",
    "\n",
    "3. **Enhanced Model with Global Bias and Weight Decay**:\n",
    "   - User and movie embeddings\n",
    "   - User and movie bias terms\n",
    "   - Global bias term\n",
    "   - Output range constraint\n",
    "   - Weight decay regularization\n",
    "   - Final validation loss: ~0.87\n",
    "\n",
    "Each enhancement has significantly improved the model's performance, reducing the validation loss by more than 3x from the basic model to the enhanced model.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "In this tutorial, we've built increasingly sophisticated collaborative filtering models for movie recommendations. We started with a simple dot product model and progressively added bias terms, output constraints, and regularization to improve performance.\n",
    "\n",
    "The key takeaways are:\n",
    "\n",
    "1. **Embeddings**: The foundation of collaborative filtering is learning meaningful embeddings for users and items.\n",
    "\n",
    "2. **Bias Terms**: Adding bias terms significantly improves performance by capturing user and item tendencies.\n",
    "\n",
    "3. **Output Constraints**: Ensuring predictions are within a valid range improves the model's practicality.\n",
    "\n",
    "4. **Regularization**: Weight decay helps prevent overfitting and improves generalization.\n",
    "\n",
    "The final model achieves a Mean Squared Error of around 0.87 on the validation set, which means that, on average, our predictions are off by about 0.93 stars (the square root of the MSE). This is quite good for a collaborative filtering model and would be practical for a real-world recommendation system.\n",
    "\n",
    "This approach can be extended in various ways, such as incorporating additional features, using more complex architectures, or applying different regularization techniques. However, even this relatively simple model provides strong performance and demonstrates the power of collaborative filtering for recommendation systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
