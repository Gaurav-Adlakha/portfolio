{
 "cells": [
  {
   "cell_type": "raw",
   "id": "e86f3716",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Vector Search and Reranking\"\n",
    "author: \"Gaurav Adlakha\"\n",
    "date: \"2025-12-22\"\n",
    "categories: [nlp, vector-search, rag, embeddings]\n",
    "description: \"Understanding the fundamentals of approximate nearest neighbor search and reranking models for semantic search applications.\"\n",
    "toc: true\n",
    "toc-depth: 2\n",
    "code-fold: true\n",
    "code-tools: true\n",
    "highlight-style: github\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0d3b0d",
   "metadata": {},
   "source": [
    "# Vector Search & Reranking: Complete Summary\n",
    "\n",
    "A comprehensive guide covering ANN, HNSW, Bi-Encoder, Cross-Encoder, ColBERT, and production RAG pipelines with sticky analogies and practical examples.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737d5482",
   "metadata": {},
   "source": [
    "## 1. Approximate Nearest Neighbor (ANN)\n",
    "\n",
    "### The Problem\n",
    "Finding similar items in massive datasets (millions of vectors) is slow with brute force â€” comparing against every single item.\n",
    "\n",
    "### ðŸ—ºï¸ Sticky Analogy: The City Map\n",
    "\n",
    "> You're at **Times Square** and need to find the **5 closest coffee shops**.\n",
    ">\n",
    "> - **Brute force**: Measure distance to every coffee shop in the city â€” takes hours.\n",
    "> - **ANN approach**: Only check coffee shops in nearby neighborhoods â€” find 5 great options in minutes.\n",
    ">\n",
    "> **Key insight**: Use the structure of space (neighborhoods) to narrow your search dramatically.\n",
    "\n",
    "### TL;DR\n",
    "- **Exact NN**: Check everything, guaranteed best, slow\n",
    "- **Approximate NN**: Check strategically, probably best, fast\n",
    "\n",
    "### One-liner to Remember\n",
    "> **\"Zoom, don't scan.\"**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455968cc",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. HNSW (Hierarchical Navigable Small World)\n",
    "\n",
    "### The Name Decoded\n",
    "- **Hierarchical** â€” multiple layers, like zoom levels\n",
    "- **Navigable** â€” you can hop/walk through it toward your target\n",
    "- **Small World** â€” few hops to reach anywhere (like \"six degrees of separation\")\n",
    "\n",
    "### ðŸ›« Sticky Analogy: Airport Network\n",
    "\n",
    "> You're in a **tiny town in rural Montana** and need to get to a **small village in southern France**.\n",
    ">\n",
    "> 1. **Top layer (major hubs)**: Think about international mega-hubs â€” JFK, Heathrow. Jump to JFK.\n",
    "> 2. **Middle layer (regional hubs)**: From JFK, find European hubs â€” Paris CDG is closest. Jump there.\n",
    "> 3. **Bottom layer (local airports)**: From Paris, find the small regional airport nearest your village.\n",
    ">\n",
    "> **You don't check every airport â€” you navigate through the hierarchy.**\n",
    "\n",
    "### ðŸ—ºï¸ Alternative Analogy: Google Maps Zoom\n",
    "\n",
    "> Finding a restaurant in Tokyo from a world view:\n",
    ">\n",
    "> **Continent â†’ Country â†’ City â†’ Street â†’ Building**\n",
    ">\n",
    "> Each zoom level has fewer things to check, but gets you closer.\n",
    "\n",
    "### ðŸŽ¯ Data Scientist Answer\n",
    "> \"HNSW builds a hierarchical graph where top layers have sparse, long-range connections for fast coarse search, and bottom layers have dense, short-range connections for precision. We start at the top, greedily navigate toward the query, and descend layer by layer. It's the algorithm behind most production vector databases.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d610ff5",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. NSW Graph Building (Proximity Graph)\n",
    "\n",
    "### How the Graph Gets Built\n",
    "\n",
    "![Navigable Small World - Proximity Graph](attachment:11b1922d-53ad-4b6d-9754-70af8513156f)\n",
    "\n",
    "**Three steps:**\n",
    "1. **Compute distances** between all document vectors\n",
    "2. **Add one node** to the graph for each document\n",
    "3. **Connect each node** to its k nearest neighbors\n",
    "\n",
    "### What is k?\n",
    "\n",
    "> **k = number of connections (edges) each node has**\n",
    ">\n",
    "> ðŸ˜ï¸ **Analogy**: 1 million houses in a city. Each house = 1 node.\n",
    "> - k=5 means each house knows 5 neighbors\n",
    "> - k=50 means each house knows 50 neighbors\n",
    "\n",
    "### k Trade-offs\n",
    "\n",
    "| Small k | Large k |\n",
    "|---------|--------|\n",
    "| Faster search | Slower search |\n",
    "| Might miss good paths | More accurate |\n",
    "| Less memory | More memory |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51df796",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Query Entry Point & Search Algorithm\n",
    "\n",
    "### Query Entry Point\n",
    "![Query Entry Point](attachment:c54869c7-b4e7-4293-aa1c-304f0301d02d)\n",
    "\n",
    "You always start from a designated **entry point node** at the top layer. From there, greedily hop to whichever neighbor is closest to your query.\n",
    "\n",
    "### Search Algorithm\n",
    "![Search Algorithm](attachment:49da41c5-6b3f-4d2e-966c-6f3ce47a3f19)\n",
    "\n",
    "**Key insight from diagram:**\n",
    "> \"May not find closest possible vectors, algorithm doesn't pick optimal overall path, just best path in each moment.\"\n",
    "\n",
    "This is why it's called **Approximate** Nearest Neighbor â€” good enough, fast enough!\n",
    "\n",
    "### The Search Process\n",
    "```\n",
    "Top layer:    Entry â†’ hop â†’ hop â†’ (can't improve) â†’ drop down\n",
    "Middle layer: â†’ hop â†’ hop â†’ (can't improve) â†’ drop down  \n",
    "Bottom layer: â†’ hop â†’ hop â†’ FOUND closest neighbors!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d4921a",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Vector Search Libraries & Databases\n",
    "\n",
    "### Most Common Algorithm\n",
    "**HNSW** â€” used by nearly every production vector search system\n",
    "\n",
    "### Libraries\n",
    "\n",
    "| Library | Creator | Notes |\n",
    "|---------|---------|-------|\n",
    "| **FAISS** | Meta | Industry standard |\n",
    "| **Annoy** | Spotify | Tree-based, simpler |\n",
    "| **ScaNN** | Google | Large-scale optimized |\n",
    "| **hnswlib** | â€” | Pure HNSW, very fast |\n",
    "\n",
    "### Vector Databases (HNSW under the hood)\n",
    "\n",
    "| Database | Notes |\n",
    "|----------|-------|\n",
    "| **Pinecone** | Fully managed, production-ready |\n",
    "| **ChromaDB** | Lightweight, great for local dev |\n",
    "| **Weaviate** | Open-source, feature-rich |\n",
    "| **Milvus** | Open-source, highly scalable |\n",
    "| **Qdrant** | Rust-based, fast |\n",
    "| **pgvector** | PostgreSQL extension |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9090500b",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Bi-Encoder\n",
    "\n",
    "### How It Works\n",
    "```\n",
    "Query  â†’  [Encoder]  â†’  Vector A\n",
    "Doc    â†’  [Encoder]  â†’  Vector B\n",
    "                          â†“\n",
    "               Compare vectors (cosine similarity)\n",
    "```\n",
    "\n",
    "Query and document encoded **separately**. The encoder never sees them together.\n",
    "\n",
    "### ðŸŽ¤ Sticky Analogy: Resume Screening\n",
    "\n",
    "> HR reads each resume independently, assigns a \"fit score\" to each.\n",
    "> - Fast â€” can process thousands quickly\n",
    "> - But never sees resume and job description *together*\n",
    "\n",
    "### Pros & Cons\n",
    "\n",
    "| Pros | Cons |\n",
    "|------|------|\n",
    "| âš¡ Very fast | Lower accuracy |\n",
    "| Pre-compute doc vectors | Misses nuanced relationships |\n",
    "| Search millions of docs | Information compressed |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8947f105",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Cross-Encoder\n",
    "\n",
    "### How It Works\n",
    "```\n",
    "[CLS] Query [SEP] Document [SEP]  â†’  Encoder  â†’  Relevance Score\n",
    "```\n",
    "\n",
    "Query and document go **together** through the model. Words can attend to each other!\n",
    "\n",
    "### ðŸŽ¤ Sticky Analogy: Job Interview\n",
    "\n",
    "> Interviewer sees candidate AND job requirements at the same time.\n",
    "> - Much more accurate â€” catches nuances\n",
    "> - But slow â€” can only do one at a time\n",
    "\n",
    "### ðŸ§ª Alternative Analogy: Chemistry\n",
    "\n",
    "> - **Bi-encoder**: Describe chemicals on paper, guess if they'll react\n",
    "> - **Cross-encoder**: Put chemicals in same beaker, observe what happens\n",
    "\n",
    "### Why Cross-Encoder is Slow\n",
    "\n",
    "| | Bi-Encoder | Cross-Encoder |\n",
    "|--|------------|---------------|\n",
    "| Query time | 1 encoder call + 1M dot products | 1M encoder calls |\n",
    "| Speed | ~milliseconds | ~hours |\n",
    "\n",
    "**You can't pre-compute** â€” it needs query and doc together.\n",
    "\n",
    "### When to Use\n",
    "**Re-ranking only** â€” top 50-100 docs from bi-encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3d0437",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-Encoder with rerankers library\n",
    "from rerankers import Reranker\n",
    "\n",
    "ranker = Reranker('cross-encoder')\n",
    "results = ranker.rank(\n",
    "    query=\"I love you\", \n",
    "    docs=[\"I hate you\", \"I really like you\"]\n",
    ")\n",
    "\n",
    "# Results:\n",
    "# \"I really like you\": -2.468 (rank 1) âœ“\n",
    "# \"I hate you\": -4.150 (rank 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a027f5cc",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. ColBERT\n",
    "\n",
    "### The Middle Ground\n",
    "\n",
    "| Model | Document Representation |\n",
    "|-------|-----------------------|\n",
    "| Bi-encoder | 1 vector per doc |\n",
    "| **ColBERT** | 1 vector per token |\n",
    "| Cross-encoder | No vectors (score directly) |\n",
    "\n",
    "### ðŸ³ Sticky Analogy: Recipe Search\n",
    "\n",
    "> **Bi-encoder**: Recipe labeled \"Italian comfort food\"\n",
    ">\n",
    "> **ColBERT**: Recipe keeps ingredients visible:\n",
    "> - [tomato] [basil] [pasta] [garlic]\n",
    ">\n",
    "> Query \"tomato pasta\" â†’ direct matches on [tomato] and [pasta] tokens!\n",
    "\n",
    "### ðŸ“± Alternative Analogy: Dating App\n",
    "\n",
    "> **Bi-encoder**: One bio sentence â€” \"I like outdoors\"\n",
    ">\n",
    "> **ColBERT**: Individual interests listed:\n",
    "> - [hiking] [photography] [camping]\n",
    ">\n",
    "> Search \"hiking photography\" â†’ direct token matches!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb13bc15",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. MaxSim Score (ColBERT's Scoring Method)\n",
    "\n",
    "![MaxSim Score Matrix](attachment:8eddb302-ddf4-437c-9795-051976ac54fd)\n",
    "\n",
    "### How It Works\n",
    "\n",
    "For each query token, find its **maximum similarity** with any doc token. Then **sum**.\n",
    "\n",
    "```\n",
    "Query: [I] [love] [you]\n",
    "Doc:   [I] [hate] [you]\n",
    "\n",
    "Similarity matrix:\n",
    "         [I]  [hate] [you]\n",
    "[I]      0.9   0.1    0.2  â†’ max = 0.9\n",
    "[love]   0.1   0.7    0.1  â†’ max = 0.7  â† \"love\" matched \"hate\"!\n",
    "[you]    0.2   0.1    0.9  â†’ max = 0.9\n",
    "\n",
    "MaxSim = 0.9 + 0.7 + 0.9 = 2.5\n",
    "```\n",
    "\n",
    "### ColBERT's Limitation\n",
    "\n",
    "\"love\" and \"hate\" get similar scores because they're both intense emotion words used in similar contexts.\n",
    "\n",
    "> **ColBERT matches tokens. Cross-encoder understands relationships.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25cb99f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ColBERT with rerankers library\n",
    "from rerankers import Reranker\n",
    "\n",
    "colbert_ranker = Reranker('colbert')\n",
    "results = colbert_ranker.rank(\n",
    "    query=\"I love you\", \n",
    "    docs=[\"I hate you\", \"I really like you\"]\n",
    ")\n",
    "\n",
    "# Results:\n",
    "# \"I hate you\": 1.753 (rank 1) âœ— Wrong!\n",
    "# \"I really like you\": 1.703 (rank 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f1ded0",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Comparison: Bi-Encoder vs Cross-Encoder vs ColBERT\n",
    "\n",
    "### ðŸ½ï¸ Restaurant Analogy\n",
    "\n",
    "| Model | Analogy |\n",
    "|-------|---------|\n",
    "| **Bi-Encoder** | Match craving to menu descriptions |\n",
    "| **ColBERT** | Match craving to ingredient lists |\n",
    "| **Cross-Encoder** | Chef serves dish knowing your craving |\n",
    "\n",
    "### Full Comparison Table\n",
    "\n",
    "| | Bi-Encoder | ColBERT | Cross-Encoder |\n",
    "|--|-----------|---------|---------------|\n",
    "| **Speed** | âš¡ Fastest | ðŸš— Medium | ðŸ¢ Slowest |\n",
    "| **Accuracy** | Good | Better | Best |\n",
    "| **Pre-compute docs?** | âœ… Yes (1 vec/doc) | âœ… Yes (1 vec/token) | âŒ No |\n",
    "| **Storage** | Low | High | N/A |\n",
    "| **Search 1M docs?** | âœ… Yes | âœ… Yes | âŒ No |\n",
    "| **Best for** | Retrieval | Retrieval + precision | Re-ranking |\n",
    "\n",
    "### When to Use Each\n",
    "\n",
    "| Scenario | Best Choice |\n",
    "|----------|-------------|\n",
    "| Search millions of docs fast | Bi-Encoder |\n",
    "| Short, dense text (tweets, titles) | ColBERT |\n",
    "| Re-rank top 50-100 results | Cross-Encoder |\n",
    "| E-commerce product search (1000s) | ColBERT |\n",
    "| Production RAG pipeline | Bi-Encoder â†’ Cross-Encoder |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51bdbff7",
   "metadata": {},
   "source": [
    "---\n",
    "## 11. HyDE (Hypothetical Document Embeddings)\n",
    "\n",
    "### The Problem\n",
    "Questions and documents \"sound\" different in embedding space.\n",
    "\n",
    "### The Trick\n",
    "1. Ask LLM to generate a **fake answer** (without retrieval)\n",
    "2. Embed the fake answer\n",
    "3. Search with that embedding instead\n",
    "\n",
    "### ðŸ  Sticky Analogy: House Hunting\n",
    "\n",
    "> You have a vague wish: \"I want something cozy with natural light.\"\n",
    ">\n",
    "> **Normal**: Tell agent your wish â†’ they struggle\n",
    ">\n",
    "> **HyDE**: Describe ideal house first: \"A 2-bedroom cottage with south-facing windows...\" â†’ Agent finds similar *actual* houses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e508572c",
   "metadata": {},
   "source": [
    "---\n",
    "## 12. Production RAG Pipeline\n",
    "\n",
    "### ðŸŽ¯ Sticky Analogy: Intern + Professor\n",
    "\n",
    "> **Intern (Retriever)**: Grabs 20 files mentioning \"Project X\" â€” some relevant, some not.\n",
    ">\n",
    "> **Professor (Reranker)**: Reads each carefully, keeps the best 5.\n",
    "\n",
    "### The Standard Two-Stage Architecture\n",
    "\n",
    "```\n",
    "User Query\n",
    "    â”‚\n",
    "    â–¼\n",
    "Stage 1: RETRIEVAL (Cast wide net)\n",
    "    â”œâ”€â”€ Vector Search (bi-encoder) \n",
    "    â””â”€â”€ Keyword Search (BM25)\n",
    "    â”‚\n",
    "    â–¼ Top 50-100 candidates\n",
    "    â”‚\n",
    "Stage 2: RERANKING\n",
    "    â””â”€â”€ Cross-Encoder scores each doc\n",
    "    â”‚\n",
    "    â–¼ Top 5-10 most relevant\n",
    "    â”‚\n",
    "Stage 3: LLM GENERATION\n",
    "    â””â”€â”€ Generate answer with context\n",
    "```\n",
    "\n",
    "### Why Hybrid Retrieval?\n",
    "- **Bi-encoder**: Semantic similarity (\"car\" â‰ˆ \"automobile\")\n",
    "- **BM25**: Exact keyword matches (\"Toyota Camry 2024\")\n",
    "\n",
    "Combined = better recall!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fcb11e6",
   "metadata": {},
   "source": [
    "---\n",
    "## 13. LLM-Based Scoring\n",
    "\n",
    "![LLM Based Scoring](attachment:990db883-cfd0-4e79-b6d0-3a6894654815)\n",
    "\n",
    "Fine-tuned LLM takes query + document â†’ outputs relevance score.\n",
    "\n",
    "**Slowest but most accurate** â€” use when quality matters more than latency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b87bf3",
   "metadata": {},
   "source": [
    "---\n",
    "## 14. Open Source Rerankers\n",
    "\n",
    "### Popular Models\n",
    "\n",
    "| Model | Creator | Speed | Notes |\n",
    "|-------|---------|-------|-------|\n",
    "| **ms-marco-MiniLM** | Microsoft | âš¡ Fastest | 22M params |\n",
    "| **mxbai-rerank** | Mixedbread | ðŸš— Medium | Default in rerankers |\n",
    "| **BGE-reranker** | BAAI | ðŸš— Medium | Most popular |\n",
    "| **ColBERTv2** | â€” | ðŸš— Medium | Token-level |\n",
    "\n",
    "### Organizations\n",
    "- **BAAI** = Beijing Academy of Artificial Intelligence\n",
    "- **Mixedbread** = German AI startup\n",
    "- **Microsoft** = MS-MARCO dataset + MiniLM architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e9633d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switching models in rerankers library\n",
    "from rerankers import Reranker\n",
    "\n",
    "# Microsoft's fast model\n",
    "ranker = Reranker('cross-encoder', model_name='cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
    "\n",
    "# BGE model (most popular)\n",
    "ranker = Reranker('cross-encoder', model_name='BAAI/bge-reranker-base')\n",
    "\n",
    "# Mixedbread (default)\n",
    "ranker = Reranker('cross-encoder', model_name='mixedbread-ai/mxbai-rerank-base-v1')\n",
    "\n",
    "# ColBERT\n",
    "ranker = Reranker('colbert', model_name='colbert-ir/colbertv2.0')\n",
    "\n",
    "# LLM-based\n",
    "ranker = Reranker('rankgpt', model_name='gpt-4o-mini')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a252d37",
   "metadata": {},
   "source": [
    "---\n",
    "## 15. Handling Long Documents\n",
    "\n",
    "### The Constraint\n",
    "Most models have **512 token limit** (~1-2 paragraphs). A 50-page doc = ~25,000 tokens.\n",
    "\n",
    "### The Solution: Chunking\n",
    "\n",
    "```\n",
    "50-page document\n",
    "       â†“\n",
    "   Chunking (~100 chunks)\n",
    "       â†“\n",
    "   Bi-encoder creates vector for each chunk\n",
    "       â†“\n",
    "   Query â†’ retrieve top 50-100 chunks\n",
    "       â†“\n",
    "   Cross-encoder re-ranks those chunks\n",
    "       â†“\n",
    "   Top 10 chunks â†’ LLM\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c2bdda",
   "metadata": {},
   "source": [
    "---\n",
    "## 16. Next steps: \n",
    "\n",
    "### Chunking Strategies\n",
    "- **Fixed Width / Recursive Character Splitting**: Good defaults\n",
    "- **Semantic / LLM Chunking**: Higher performance, more complex\n",
    "- **Context-Aware Chunking**: Good \"first improvement\" to explore\n",
    "\n",
    "### Query Enhancement\n",
    "- **Query Parsing**\n",
    "- **NER (Named Entity Recognition)** for query parsing\n",
    "- **HyDE** for better query-document matching\n",
    "\n",
    "### Advanced Ideas\n",
    "- Use **ColBERT on subject lines**, **Bi-encoder on body** (hybrid approach)\n",
    "- **Rank fusion** combining multiple retrieval methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe90535",
   "metadata": {},
   "source": [
    "---\n",
    "## 17. Real-World ColBERT Use Case\n",
    "\n",
    "### E-commerce Product Search (eBay, Amazon)\n",
    "\n",
    "Product titles are short and every word matters:\n",
    "- \"iPhone 15 Pro Max 256GB Blue Unlocked\"\n",
    "- \"Nike Air Jordan 1 Retro High OG Chicago\"\n",
    "\n",
    "**Why ColBERT works:**\n",
    "1. Titles are short â€” every token matters\n",
    "2. Exact terms matter â€” \"256GB\" must match \"256GB\"\n",
    "3. Need to re-rank 1000s of products quickly\n",
    "\n",
    "```\n",
    "Query: \"Nike Jordan 1 Chicago size 10\"\n",
    "    â†“\n",
    "Bi-encoder retrieves top 1000 products\n",
    "    â†“\n",
    "ColBERT re-ranks (fast, token-level)\n",
    "    â†“\n",
    "Top 50 shown to user\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5515fea",
   "metadata": {},
   "source": [
    "---\n",
    "# Quick Reference\n",
    "\n",
    "## One-Liners to Remember\n",
    "\n",
    "| Concept | One-liner |\n",
    "|---------|----------|\n",
    "| ANN | \"Zoom, don't scan\" |\n",
    "| HNSW | \"Multi-layer graph, start sparse, end dense\" |\n",
    "| Bi-encoder | \"Encode separately, compare vectors\" |\n",
    "| Cross-encoder | \"Encode together, understand relationship\" |\n",
    "| ColBERT | \"Token-level matching with MaxSim\" |\n",
    "\n",
    "## When to Use What\n",
    "\n",
    "| Task | Solution |\n",
    "|------|----------|\n",
    "| Search 1M docs | Bi-encoder + HNSW |\n",
    "| Re-rank top 50-100 | Cross-encoder |\n",
    "| Re-rank 500-1000 | ColBERT |\n",
    "| Short dense text | ColBERT |\n",
    "| Maximum quality | LLM-based reranker |\n",
    "| Long documents | Chunk first! |\n",
    "\n",
    "## Speed Ranking\n",
    "```\n",
    "Bi-encoder âš¡ > ColBERT ðŸš— > Cross-encoder ðŸ¢ > LLM-based ðŸŒ\n",
    "```\n",
    "\n",
    "## Accuracy Ranking\n",
    "```\n",
    "LLM-based ðŸŽ¯ > Cross-encoder > ColBERT > Bi-encoder\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b73a671",
   "metadata": {
    "time_run": "2025-12-22T09:33:03.602242+00:00"
   },
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "from dialoghelper import curr_dialog\n",
    "\n",
    "def deploy_notebook():\n",
    "    nb_name = Path(curr_dialog()['name']).name + '.ipynb'\n",
    "    src = f'/app/data/{curr_dialog()[\"name\"]}.ipynb'\n",
    "    dst = '/app/data/publish/portfolio/static/'\n",
    "    print(nb_name)\n",
    "    \n",
    "    # Copy notebook to static folder\n",
    "    subprocess.run(['cp', src, dst])\n",
    "    \n",
    "    # Deploy with plash\n",
    "    subprocess.run(['plash_deploy'], cwd='/app/data/publish/portfolio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020b542a",
   "metadata": {
    "time_run": "2025-12-22T09:33:11.826922+00:00"
   },
   "outputs": [],
   "source": [
    "#| hide\n",
    "deploy_notebook()"
   ]
  }
 ],
 "metadata": {
  "solveit_dialog_mode": "learning",
  "solveit_ver": 2
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
