{
 "cells": [
  {
   "cell_type": "raw",
   "id": "247d87f5",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Understanding Transformers Through a Networking Party Analogy\"\n",
    "author: \"Gaurav Adlakha\"\n",
    "date: 2025-12-31\n",
    "categories: [transformers, deep-learning, pytorch, nlp]\n",
    "description: \"Learn transformer architecture through an intuitive networking party analogy â€” from embeddings to multi-head attention to full transformer blocks.\"\n",
    "toc: true\n",
    "toc-depth: 2\n",
    "code-fold: true\n",
    "code-tools: true\n",
    "highlight-style: github\n",
    "fig-width: 8\n",
    "fig-height: 6\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de520016",
   "metadata": {},
   "source": [
    "# Transformer Architecture: A Complete Guide\n",
    "\n",
    "## The Networking Event Story ðŸŽ‰\n",
    "\n",
    "This notebook captures our exploration of transformer architecture through an intuitive **networking event analogy**. Every component maps to a part of attending professional networking parties.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abda159",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Token & Positional Embeddings](#1-token--positional-embeddings)\n",
    "2. [Self-Attention (Q, K, V)](#2-self-attention-q-k-v)\n",
    "3. [Causal Masking](#3-causal-masking)\n",
    "4. [Multi-Head Attention](#4-multi-head-attention)\n",
    "5. [Layer Normalization](#5-layer-normalization)\n",
    "6. [Feed-Forward Network (MLP)](#6-feed-forward-network-mlp)\n",
    "7. [Residual Connections](#7-residual-connections)\n",
    "8. [Transformer Block](#8-transformer-block)\n",
    "9. [Ready-Made Options](#9-ready-made-options)\n",
    "10. [Quick Reference](#10-quick-reference)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b72fce4",
   "metadata": {},
   "source": [
    "## 1. Token & Positional Embeddings\n",
    "\n",
    "### The Problem\n",
    "How does a transformer convert raw token indices into meaningful vectors that also know their position in a sequence?\n",
    "\n",
    "### The Solution\n",
    "Two embedding lookup tables, **added together**:\n",
    "1. **Token embedding** â€” maps each word to a learned vector (what the word *means*)\n",
    "2. **Positional embedding** â€” maps each position to a learned vector (where the word *sits*)\n",
    "\n",
    "> **ðŸŽ¯ Sticky Analogy:** Imagine name tags at a dinner party. The token embedding is your *name*, the positional embedding is your *seat number*. Combined, everyone knows both who you are and where you're sitting.\n",
    "\n",
    "### Key Insight\n",
    "- Embeddings are just **learnable lookup tables** (matrices)\n",
    "- Token + position embeddings are **added**, not concatenated\n",
    "- This combined vector is what enters the transformer layers\n",
    "\n",
    "### GPT-2 vs Our Toy Model\n",
    "\n",
    "| | Toy Model | GPT-2 |\n",
    "|---|-----------|-------|\n",
    "| Token embedding | (5, 3) | (50257, 768) |\n",
    "| Position embedding | (10, 3) | (1024, 768) |\n",
    "| Vocab size | 5 words | 50,257 tokens |\n",
    "| Max sequence | 10 | 1,024 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378a7b4d",
   "metadata": {
    "time_run": "2025-12-31T14:55:02.544834+00:00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined shape: torch.Size([4, 3])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Token embedding: what each word means\n",
    "vocab_size, embed_dim = 5, 3\n",
    "embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "\n",
    "# Position embedding: where each word sits\n",
    "max_seq_len = 10\n",
    "pos_embedding = nn.Embedding(max_seq_len, embed_dim)\n",
    "\n",
    "# \"cat sat on mat\" â†’ indices [0, 2, 3, 4]\n",
    "sentence = torch.tensor([0, 2, 3, 4])\n",
    "positions = torch.tensor([0, 1, 2, 3])\n",
    "\n",
    "# Combined: token + position (element-wise addition)\n",
    "combined = embedding(sentence) + pos_embedding(positions)\n",
    "print(f\"Combined shape: {combined.shape}\")  # (4, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e5cb8a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Self-Attention (Q, K, V)\n",
    "\n",
    "### The Problem\n",
    "How can each word in a sentence learn from other relevant words to understand context?\n",
    "\n",
    "### The Solution\n",
    "Self-attention mechanism with **Q, K, V projections**:\n",
    "- **Q (Query)** â€” \"What am I looking for?\"\n",
    "- **K (Key)** â€” \"What do I advertise?\"\n",
    "- **V (Value)** â€” \"What can I share?\"\n",
    "\n",
    "> **ðŸŽ¯ Sticky Analogy: The Q/K/V Dance**\n",
    "> \n",
    "> Imagine a networking party:\n",
    "> - **Q**: \"Who should I talk to?\" (asking questions)\n",
    "> - **K**: \"Here's my business card\" (showing name tags)\n",
    "> - **V**: \"Here's my expertise to share\" (sharing info)\n",
    "> \n",
    "> Everyone does the Q/K/V dance with everyone else!\n",
    "\n",
    "### The Flow\n",
    "1. Compute scores: `Q @ K.T` â†’ \"Who is relevant to me?\"\n",
    "2. Softmax â†’ Convert to percentages (attention weights)\n",
    "3. Weighted sum: `attn_weights @ V` â†’ Absorb information from relevant words\n",
    "\n",
    "### Key Concept: Learned Projections\n",
    "Q, K, V are **not** raw embeddings â€” they're transformed by **learned projection matrices** (W_q, W_k, W_v). Through training:\n",
    "- W_q learns: \"Given my embedding, what should I **look for**?\"\n",
    "- W_k learns: \"Given my embedding, what should I **advertise**?\"\n",
    "- W_v learns: \"Given my embedding, what should I **share**?\"\n",
    "\n",
    "> **ðŸ’¡ Key Insight:** Words start **isolated** (can't talk to each other). After attention, each word has **absorbed context** from relevant words, producing a **new representation**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9254aa65",
   "metadata": {
    "time_run": "2025-12-31T14:55:02.547793+00:00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights shape: torch.Size([4, 4])\n",
      "Output shape: torch.Size([4, 3])\n"
     ]
    }
   ],
   "source": [
    "# Q, K, V projections (learned during training)\n",
    "d_model = 3\n",
    "W_q = nn.Linear(d_model, d_model, bias=False)\n",
    "W_k = nn.Linear(d_model, d_model, bias=False)\n",
    "W_v = nn.Linear(d_model, d_model, bias=False)\n",
    "\n",
    "Q = W_q(combined)\n",
    "K = W_k(combined)\n",
    "V = W_v(combined)\n",
    "\n",
    "# Attention scores: \"How relevant is each word to each other?\"\n",
    "scores = Q @ K.T  # (4, 4)\n",
    "\n",
    "# Convert to percentages\n",
    "attn_weights = F.softmax(scores, dim=-1)\n",
    "\n",
    "# New representation: absorb information from others\n",
    "output = attn_weights @ V\n",
    "print(f\"Attention weights shape: {attn_weights.shape}\")  # (4, 4)\n",
    "print(f\"Output shape: {output.shape}\")  # (4, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3bc080",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Causal Masking\n",
    "\n",
    "### The Problem\n",
    "In language generation, words shouldn't peek at future words. How do we prevent this?\n",
    "\n",
    "### The Solution\n",
    "Apply a **lower-triangular mask** before softmax:\n",
    "1. Create mask: `torch.tril(torch.ones(seq_len, seq_len))`\n",
    "2. Set future positions to `-inf`: `scores.masked_fill(mask == 0, float('-inf'))`\n",
    "3. Softmax converts `-inf` â†’ 0% attention\n",
    "\n",
    "> **ðŸŽ¯ Sticky Analogy:** It's like reading a mystery novel â€” you can remember what you've already read (past tokens), but you can't flip ahead to see who the killer is (future tokens).\n",
    "\n",
    "### The Mask Pattern\n",
    "\n",
    "| | cat | sat | on | mat |\n",
    "|---|---|---|---|---|\n",
    "| **cat** | âœ“ | âœ— | âœ— | âœ— |\n",
    "| **sat** | âœ“ | âœ“ | âœ— | âœ— |\n",
    "| **on** | âœ“ | âœ“ | âœ“ | âœ— |\n",
    "| **mat** | âœ“ | âœ“ | âœ“ | âœ“ |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728ab976",
   "metadata": {
    "time_run": "2025-12-31T14:55:02.581801+00:00"
   },
   "outputs": [],
   "source": [
    "# Create causal mask (lower triangular)\n",
    "mask = torch.tril(torch.ones(4, 4))\n",
    "\n",
    "# Apply mask: future positions â†’ -inf\n",
    "scores_masked = scores.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "# Softmax: -inf becomes 0%\n",
    "attn_weights_causal = F.softmax(scores_masked, dim=-1)\n",
    "\n",
    "# Now words can only attend to past + self\n",
    "output_causal = attn_weights_causal @ V"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c22527",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Multi-Head Attention\n",
    "\n",
    "### The Problem\n",
    "Single-head attention produces only **one attention pattern** â€” it can focus on one type of relationship at a time. But language has multiple simultaneous relationships (grammar, coreference, semantics, etc.).\n",
    "\n",
    "### The Solution\n",
    "Split the embedding into **multiple heads**, each running its own attention in parallel:\n",
    "- 768 dims Ã· 12 heads = 64 dims per head\n",
    "- Each head learns different relationships\n",
    "- Concatenate outputs back to 768 dims\n",
    "\n",
    "> **ðŸŽ¯ Sticky Analogy:** 12 detectives investigating the same crime scene. Each has a specialty (fingerprints, timelines, witness relationships). They work **in parallel** and combine their findings into one complete report.\n",
    "\n",
    "### Key Term: **Expressiveness**\n",
    "Multi-head attention gives the model more **expressiveness** â€” the ability to capture multiple types of relationships simultaneously.\n",
    "\n",
    "### Heads vs Layers\n",
    "\n",
    "| | Multi-Head | Layers |\n",
    "|---|-----------|--------|\n",
    "| **What** | 12 heads | 12 blocks |\n",
    "| **Direction** | Parallel (side by side) | Sequential (one after another) |\n",
    "| **Purpose** | Expressiveness â€” different perspectives at once | Depth â€” build understanding layer by layer |\n",
    "| **Analogy** | 12 conversations at the **same** party | 12 **different** parties over time |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07730a1",
   "metadata": {
    "time_run": "2025-12-31T14:55:02.615007+00:00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: torch.Size([1, 4, 3])\n",
      "Output: torch.Size([1, 4, 3])\n",
      "Attention: torch.Size([1, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "# Using PyTorch's Multi-Head Attention\n",
    "mha = nn.MultiheadAttention(embed_dim=3, num_heads=3, batch_first=True)\n",
    "\n",
    "# Add batch dimension\n",
    "x_batched = combined.unsqueeze(0)  # (4, 3) â†’ (1, 4, 3)\n",
    "\n",
    "# Causal mask for PyTorch\n",
    "attn_mask = torch.triu(torch.ones(4, 4), diagonal=1).bool()\n",
    "\n",
    "# Self-attention: pass x three times (Q, K, V all same)\n",
    "output, attn_weights = mha(x_batched, x_batched, x_batched, attn_mask=attn_mask)\n",
    "\n",
    "print(f\"Input: {x_batched.shape}\")       # (1, 4, 3)\n",
    "print(f\"Output: {output.shape}\")          # (1, 4, 3)\n",
    "print(f\"Attention: {attn_weights.shape}\") # (1, 4, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95193c54",
   "metadata": {},
   "source": [
    "### The Attention Dance (Complete Code with Analogies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500cffd5",
   "metadata": {
    "time_run": "2025-12-31T14:55:02.647968+00:00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before dance (isolated): torch.Size([1, 4, 3])\n",
      "After dance (absorbed context): torch.Size([1, 4, 3])\n"
     ]
    }
   ],
   "source": [
    "# THE ATTENTION DANCE ðŸ’ƒ\n",
    "\n",
    "# 4 people at a party, each with a 6-number ID card\n",
    "people_at_party = combined.unsqueeze(0)  # (1, 4, 3) = (1 party, 4 people, 3-number ID)\n",
    "\n",
    "# The dance floor (3 heads = 3 different conversations)\n",
    "dance_floor = nn.MultiheadAttention(embed_dim=3, num_heads=3, batch_first=True)\n",
    "\n",
    "# Causal mask: can only talk to people you've already met (no peeking ahead!)\n",
    "no_peeking = torch.triu(torch.ones(4, 4), diagonal=1).bool()\n",
    "\n",
    "# Everyone does the Q/K/V dance with everyone else (self-attention)\n",
    "# Q: \"Who should I talk to?\"  K: \"Here's my name tag\"  V: \"Here's what I'll share\"\n",
    "new_representation, who_talked_to_whom = dance_floor(\n",
    "    people_at_party,  # Q - asking questions\n",
    "    people_at_party,  # K - showing name tags  \n",
    "    people_at_party,  # V - sharing info\n",
    "    attn_mask=no_peeking\n",
    ")\n",
    "\n",
    "print(f\"Before dance (isolated): {people_at_party.shape}\")\n",
    "print(f\"After dance (absorbed context): {new_representation.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eab24c5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Layer Normalization\n",
    "\n",
    "### The Problem\n",
    "After attention, some values might be very large (\"shouting\") and some very small (\"whispering\"). This makes training unstable.\n",
    "\n",
    "### The Solution\n",
    "Normalize each vector to have **mean=0** and **std=1**.\n",
    "\n",
    "> **ðŸŽ¯ Sticky Analogy:** The **sound engineer** at the party â€” adjusts everyone's volume so they're all speaking at a similar level.\n",
    "\n",
    "### What LayerNorm Does\n",
    "1. **Centers** the values â†’ mean becomes 0\n",
    "2. **Scales** the values â†’ std becomes 1\n",
    "\n",
    "Also has learnable parameters (gamma, beta) for fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7502d51b",
   "metadata": {
    "time_run": "2025-12-31T14:55:02.678158+00:00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before LayerNorm:\n",
      "tensor([[ 0.5131,  0.5449,  0.3283],\n",
      "        [ 0.4041,  0.9956,  0.4095],\n",
      "        [-0.1467, -0.8052, -0.4480],\n",
      "        [ 0.6005,  0.5295,  0.0015]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "After LayerNorm:\n",
      "tensor([[ 0.5334,  0.8669, -1.4003],\n",
      "        [-0.7169,  1.4141, -0.6971],\n",
      "        [ 1.1885, -1.2579,  0.0694],\n",
      "        [ 0.8358,  0.5700, -1.4058]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "Per-person mean: tensor([ 0.0000e+00, -1.9868e-08, -6.4572e-08, -3.9736e-08])\n",
      "Per-person std:  tensor([1.2241, 1.2247, 1.2247, 1.2247])\n"
     ]
    }
   ],
   "source": [
    "# The sound engineer â€” normalizes across the embedding dimension\n",
    "sound_engineer = nn.LayerNorm(3)\n",
    "\n",
    "# Before: chaotic volumes\n",
    "print(\"Before LayerNorm:\")\n",
    "print(new_representation[0])\n",
    "\n",
    "# After: balanced volumes\n",
    "normalized = sound_engineer(new_representation)\n",
    "print(\"\\nAfter LayerNorm:\")\n",
    "print(normalized[0])\n",
    "\n",
    "# Each person now has meanâ‰ˆ0, stdâ‰ˆ1\n",
    "print(f\"\\nPer-person mean: {normalized[0].mean(dim=-1).detach()}\")\n",
    "print(f\"Per-person std:  {normalized[0].std(dim=-1).detach()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b74c88",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Feed-Forward Network (MLP)\n",
    "\n",
    "### The Problem\n",
    "After attention, each person has absorbed contacts. But raw contacts aren't enough â€” they need to **process and reflect** on what they learned.\n",
    "\n",
    "### The Solution\n",
    "**Expand then compress**: think deeply, extract key insights.\n",
    "- Expand: 768 â†’ 3072 (4Ã— larger)\n",
    "- Activation: GELU (\"aha moments\")\n",
    "- Compress: 3072 â†’ 768 (back to original)\n",
    "\n",
    "> **ðŸŽ¯ Sticky Analogy:** Going home and **journaling** after the party. First you expand your thoughts (think deeply, explore ideas), then you compress back to key insights.\n",
    "\n",
    "### Why GELU?\n",
    "- **ReLU**: Hard cutoff at 0. Anything negative â†’ 0. Harsh.\n",
    "- **GELU**: Smooth curve. Small negatives get partially kept. Better gradient flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6dd1ae",
   "metadata": {
    "time_run": "2025-12-31T14:55:02.710062+00:00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanded thoughts: torch.Size([1, 4, 12])\n",
      "Compressed insights: torch.Size([1, 4, 3])\n"
     ]
    }
   ],
   "source": [
    "# After the party, time to reflect and process (MLP)\n",
    "expand_thoughts = nn.Linear(3, 12)   # Think deeply (3 â†’ 12, i.e., 4Ã—)\n",
    "compress_insights = nn.Linear(12, 3) # Key insights (12 â†’ 3)\n",
    "\n",
    "reflected = expand_thoughts(normalized)\n",
    "print(f\"Expanded thoughts: {reflected.shape}\")  # (1, 4, 12)\n",
    "\n",
    "reflected = F.gelu(reflected)  # Activation: \"aha moments\"\n",
    "\n",
    "reflected = compress_insights(reflected)\n",
    "print(f\"Compressed insights: {reflected.shape}\")  # (1, 4, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faff426a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Residual Connections\n",
    "\n",
    "### The Problem\n",
    "After reflecting in your journal (MLP), you now have \"processed insights.\" But what if you overthink and **forget who you originally were**?\n",
    "\n",
    "### The Solution\n",
    "Keep your original identity while adding the new stuff:\n",
    "\n",
    "```python\n",
    "output = original + processed\n",
    "```\n",
    "\n",
    "> **ðŸŽ¯ Sticky Analogy:** Keep your original **ID card** while adding new learnings on top. **Don't forget yourself!** Even after many rounds of parties (transformer layers), you retain your core identity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81eb976d",
   "metadata": {
    "time_run": "2025-12-31T14:55:02.743319+00:00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: torch.Size([1, 4, 3])\n",
      "Final (original + learning): torch.Size([1, 4, 3])\n"
     ]
    }
   ],
   "source": [
    "# Residual connection: keep your original identity\n",
    "original = normalized\n",
    "processed = reflected\n",
    "\n",
    "# Don't lose yourself! Original + new learning\n",
    "final = original + processed\n",
    "print(f\"Original: {original.shape}\")\n",
    "print(f\"Final (original + learning): {final.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf018d2b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Transformer Block\n",
    "\n",
    "### The Complete Networking Event\n",
    "\n",
    "One Transformer Block = One complete networking event cycle:\n",
    "\n",
    "1. **Arrive at party** â†’ Input embeddings\n",
    "2. **Mingle & dance** â†’ Multi-Head Attention (Q/K/V dance)\n",
    "3. **Don't forget yourself** â†’ Residual connection\n",
    "4. **Sound engineer** â†’ LayerNorm\n",
    "5. **Go home & journal** â†’ MLP (expand â†’ aha â†’ compress)\n",
    "6. **Don't forget yourself again** â†’ Residual connection\n",
    "7. **Sound engineer again** â†’ LayerNorm\n",
    "\n",
    "### GPT-2 Architecture\n",
    "- **12 layers** (sequential) â€” 12 different parties over time\n",
    "- **12 heads per layer** (parallel) â€” 12 conversations at each party\n",
    "- Total: 12 Ã— 12 = **144 attention patterns**!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3918cd80",
   "metadata": {
    "time_run": "2025-12-31T14:55:02.809752+00:00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: torch.Size([1, 4, 3])\n",
      "Output: torch.Size([1, 4, 3])\n"
     ]
    }
   ],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, n_heads, mlp_ratio=4):\n",
    "        super().__init__()\n",
    "        # The dance floor (multi-head attention)\n",
    "        self.attention = nn.MultiheadAttention(embed_dim, n_heads, batch_first=True)\n",
    "        \n",
    "        # Sound engineers (layer norms)\n",
    "        self.ln1 = nn.LayerNorm(embed_dim)\n",
    "        self.ln2 = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "        # Journaling room (MLP: expand â†’ aha â†’ compress)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim * mlp_ratio),  # Expand thoughts\n",
    "            nn.GELU(),                                     # Aha moments\n",
    "            nn.Linear(embed_dim * mlp_ratio, embed_dim)   # Compress insights\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        # 1. Attention dance + don't forget yourself (residual)\n",
    "        attn_out, _ = self.attention(x, x, x, attn_mask=mask)\n",
    "        x = self.ln1(x + attn_out)  # Residual + sound engineer\n",
    "        \n",
    "        # 2. Journaling + don't forget yourself (residual)\n",
    "        mlp_out = self.mlp(x)\n",
    "        x = self.ln2(x + mlp_out)   # Residual + sound engineer\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Test it\n",
    "block = TransformerBlock(embed_dim=3, n_heads=3)\n",
    "causal_mask = torch.triu(torch.ones(4, 4), diagonal=1).bool()\n",
    "output = block(people_at_party, mask=causal_mask)\n",
    "print(f\"Input: {people_at_party.shape}\")\n",
    "print(f\"Output: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e88b262",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Ready-Made Options\n",
    "\n",
    "### Three Ways to Build Transformers\n",
    "\n",
    "| Approach | Use Case | Analogy |\n",
    "|----------|----------|--------|\n",
    "| **Build from scratch** | Learning, research | Learning how engines work |\n",
    "| **PyTorch built-in** | Custom architectures | Buying engine parts, assembling yourself |\n",
    "| **HuggingFace** | Production apps | Buying a complete car, ready to drive |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e397e32",
   "metadata": {
    "time_run": "2025-12-31T14:55:03.057903+00:00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: cat sat on mat\n",
      "Token IDs: tensor([[9246, 3332,  319, 2603]])\n"
     ]
    }
   ],
   "source": [
    "# Option 1: Our custom block (what we built)\n",
    "block = TransformerBlock(embed_dim=3, n_heads=3)\n",
    "\n",
    "# Option 2: PyTorch ready-made\n",
    "decoder_layer = nn.TransformerDecoderLayer(\n",
    "    d_model=3, \n",
    "    nhead=3, \n",
    "    dim_feedforward=12, \n",
    "    batch_first=True\n",
    ")\n",
    "\n",
    "# Option 3: HuggingFace (full GPT-2, pre-trained!)\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "# Tokenize text\n",
    "text = \"cat sat on mat\"\n",
    "tokens = tokenizer(text, return_tensors='pt')\n",
    "print(f\"Text: {text}\")\n",
    "print(f\"Token IDs: {tokens['input_ids']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22941c7b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10. Quick Reference\n",
    "\n",
    "### The Complete Networking Event Story\n",
    "\n",
    "| Component | Analogy | What it does |\n",
    "|-----------|---------|-------------|\n",
    "| Token Embedding | ID card (who you are) | Maps words â†’ vectors |\n",
    "| Position Embedding | Seat number | Encodes word order |\n",
    "| Multi-Head Attention | Q/K/V dance at the party | Words absorb context from each other |\n",
    "| Causal Mask | No peeking ahead (mystery novel) | Can only see past tokens |\n",
    "| LayerNorm | Sound engineer | Balance volumes (mean=0, std=1) |\n",
    "| MLP | Journaling at home | Expand â†’ aha â†’ compress insights |\n",
    "| Residual Connection | Don't forget yourself | Original + learning |\n",
    "| Transformer Block | One networking event | Attention + MLP + residuals |\n",
    "| 12 Layers | 12 parties over time | Deeper understanding |\n",
    "| 12 Heads | 12 conversations at same party | Expressiveness |\n",
    "\n",
    "### GPT-2 Architecture Numbers\n",
    "\n",
    "| Parameter | GPT-2 Small | GPT-2 Medium | GPT-2 Large |\n",
    "|-----------|-------------|--------------|-------------|\n",
    "| Layers | 12 | 24 | 36 |\n",
    "| Heads | 12 | 16 | 20 |\n",
    "| Embed dim | 768 | 1024 | 1280 |\n",
    "| Vocab size | 50,257 | 50,257 | 50,257 |\n",
    "| Max seq len | 1,024 | 1,024 | 1,024 |\n",
    "\n",
    "### Key Formulas\n",
    "\n",
    "**Self-Attention:**\n",
    "\\[\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
    "\\]\n",
    "\n",
    "**Layer Normalization:**\n",
    "\\[\n",
    "\\text{LayerNorm}(x) = \\gamma \\cdot \\frac{x - \\mu}{\\sigma} + \\beta\n",
    "\\]\n",
    "\n",
    "**Residual Connection:**\n",
    "\\[\n",
    "\\text{output} = x + \\text{SubLayer}(x)\n",
    "\\]\n",
    "\n",
    "### Tensor Shapes Cheatsheet\n",
    "\n",
    "| Tensor | Shape | Description |\n",
    "|--------|-------|-------------|\n",
    "| Input tokens | (batch, seq_len) | Token indices |\n",
    "| Embeddings | (batch, seq_len, embed_dim) | After token + position |\n",
    "| Q, K, V | (batch, seq_len, embed_dim) | Projections |\n",
    "| Attention weights | (batch, seq_len, seq_len) | Who attends to whom |\n",
    "| Output | (batch, seq_len, embed_dim) | New representation |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce716840",
   "metadata": {
    "time_run": "2025-12-31T14:55:03.510453+00:00"
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.43.0 (0)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"369pt\" height=\"825pt\"\n",
       " viewBox=\"0.00 0.00 369.00 825.39\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 821.39)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-821.39 365,-821.39 365,4 -4,4\"/>\n",
       "<!-- tok -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>tok</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"183,-817.39 1,-817.39 1,-779.39 183,-779.39 183,-817.39\"/>\n",
       "<text text-anchor=\"middle\" x=\"92\" y=\"-802.19\" font-family=\"Times,serif\" font-size=\"14.00\">Token Embedding</text>\n",
       "<text text-anchor=\"middle\" x=\"92\" y=\"-787.19\" font-family=\"Times,serif\" font-size=\"14.00\">(what the word means)</text>\n",
       "</g>\n",
       "<!-- combined -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>combined</title>\n",
       "<polygon fill=\"lightgreen\" stroke=\"black\" points=\"278.5,-742.39 93.5,-742.39 93.5,-704.39 278.5,-704.39 278.5,-742.39\"/>\n",
       "<text text-anchor=\"middle\" x=\"186\" y=\"-727.19\" font-family=\"Times,serif\" font-size=\"14.00\">Combined</text>\n",
       "<text text-anchor=\"middle\" x=\"186\" y=\"-712.19\" font-family=\"Times,serif\" font-size=\"14.00\">(isolated, can&#39;t talk yet)</text>\n",
       "</g>\n",
       "<!-- tok&#45;&gt;combined -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>tok&#45;&gt;combined</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M115.24,-779.35C127.1,-770.14 141.7,-758.79 154.56,-748.81\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"156.95,-751.38 162.71,-742.48 152.66,-745.85 156.95,-751.38\"/>\n",
       "</g>\n",
       "<!-- pos -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>pos</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"361,-817.39 201,-817.39 201,-779.39 361,-779.39 361,-817.39\"/>\n",
       "<text text-anchor=\"middle\" x=\"281\" y=\"-802.19\" font-family=\"Times,serif\" font-size=\"14.00\">Position Embedding</text>\n",
       "<text text-anchor=\"middle\" x=\"281\" y=\"-787.19\" font-family=\"Times,serif\" font-size=\"14.00\">(where it sits)</text>\n",
       "</g>\n",
       "<!-- pos&#45;&gt;combined -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>pos&#45;&gt;combined</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M257.52,-779.35C245.53,-770.14 230.77,-758.79 217.78,-748.81\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"219.6,-745.8 209.54,-742.48 215.34,-751.35 219.6,-745.8\"/>\n",
       "</g>\n",
       "<!-- block_start -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>block_start</title>\n",
       "<polygon fill=\"white\" stroke=\"black\" stroke-width=\"2\" points=\"281.5,-667.39 90.5,-667.39 90.5,-629.39 281.5,-629.39 281.5,-667.39\"/>\n",
       "<text text-anchor=\"middle\" x=\"186\" y=\"-652.19\" font-family=\"Times,serif\" font-size=\"14.00\">TRANSFORMER BLOCK</text>\n",
       "<text text-anchor=\"middle\" x=\"186\" y=\"-637.19\" font-family=\"Times,serif\" font-size=\"14.00\">(one networking event)</text>\n",
       "</g>\n",
       "<!-- combined&#45;&gt;block_start -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>combined&#45;&gt;block_start</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M186,-704.35C186,-696.28 186,-686.57 186,-677.58\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"189.5,-677.48 186,-667.48 182.5,-677.48 189.5,-677.48\"/>\n",
       "</g>\n",
       "<!-- mha -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>mha</title>\n",
       "<polygon fill=\"lightyellow\" stroke=\"black\" points=\"224,-592.39 0,-592.39 0,-539.39 224,-539.39 224,-592.39\"/>\n",
       "<text text-anchor=\"middle\" x=\"112\" y=\"-577.19\" font-family=\"Times,serif\" font-size=\"14.00\">Multi&#45;Head Attention</text>\n",
       "<text text-anchor=\"middle\" x=\"112\" y=\"-562.19\" font-family=\"Times,serif\" font-size=\"14.00\">(Q/K/V dance)</text>\n",
       "<text text-anchor=\"middle\" x=\"112\" y=\"-547.19\" font-family=\"Times,serif\" font-size=\"14.00\">12 heads = 12 conversations</text>\n",
       "</g>\n",
       "<!-- block_start&#45;&gt;mha -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>block_start&#45;&gt;mha</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M169.21,-629.13C161.26,-620.48 151.51,-609.87 142.33,-599.89\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"144.8,-597.4 135.46,-592.41 139.65,-602.14 144.8,-597.4\"/>\n",
       "</g>\n",
       "<!-- res1 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>res1</title>\n",
       "<ellipse fill=\"lightgray\" stroke=\"black\" cx=\"186\" cy=\"-483.54\" rx=\"18.7\" ry=\"18.7\"/>\n",
       "<text text-anchor=\"middle\" x=\"186\" y=\"-479.84\" font-family=\"Times,serif\" font-size=\"14.00\">+</text>\n",
       "</g>\n",
       "<!-- block_start&#45;&gt;res1 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>block_start&#45;&gt;res1</title>\n",
       "<path fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" d=\"M207.28,-629.29C217.13,-619.5 227.78,-606.53 233,-592.39 241.16,-570.29 241.17,-561.49 233,-539.39 227.92,-525.66 217.69,-513.04 208.07,-503.38\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"210.27,-500.63 200.6,-496.3 205.45,-505.72 210.27,-500.63\"/>\n",
       "<text text-anchor=\"middle\" x=\"287.5\" y=\"-569.69\" font-family=\"Times,serif\" font-size=\"14.00\">residual</text>\n",
       "<text text-anchor=\"middle\" x=\"287.5\" y=\"-554.69\" font-family=\"Times,serif\" font-size=\"14.00\">(don&#39;t forget)</text>\n",
       "</g>\n",
       "<!-- mha&#45;&gt;res1 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>mha&#45;&gt;res1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M135.82,-539.03C145.69,-528.31 157,-516.04 166.38,-505.85\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"169.22,-507.93 173.42,-498.2 164.07,-503.19 169.22,-507.93\"/>\n",
       "</g>\n",
       "<!-- ln1 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>ln1</title>\n",
       "<polygon fill=\"lightpink\" stroke=\"black\" points=\"254.5,-427.7 117.5,-427.7 117.5,-389.7 254.5,-389.7 254.5,-427.7\"/>\n",
       "<text text-anchor=\"middle\" x=\"186\" y=\"-412.5\" font-family=\"Times,serif\" font-size=\"14.00\">LayerNorm</text>\n",
       "<text text-anchor=\"middle\" x=\"186\" y=\"-397.5\" font-family=\"Times,serif\" font-size=\"14.00\">(sound engineer)</text>\n",
       "</g>\n",
       "<!-- res1&#45;&gt;ln1 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>res1&#45;&gt;ln1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M186,-464.54C186,-456.48 186,-446.8 186,-437.82\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"189.5,-437.75 186,-427.75 182.5,-437.75 189.5,-437.75\"/>\n",
       "</g>\n",
       "<!-- mlp -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>mlp</title>\n",
       "<polygon fill=\"lightyellow\" stroke=\"black\" points=\"219,-352.7 15,-352.7 15,-299.7 219,-299.7 219,-352.7\"/>\n",
       "<text text-anchor=\"middle\" x=\"117\" y=\"-337.5\" font-family=\"Times,serif\" font-size=\"14.00\">MLP</text>\n",
       "<text text-anchor=\"middle\" x=\"117\" y=\"-322.5\" font-family=\"Times,serif\" font-size=\"14.00\">(journaling)</text>\n",
       "<text text-anchor=\"middle\" x=\"117\" y=\"-307.5\" font-family=\"Times,serif\" font-size=\"14.00\">expand â†’ aha â†’ compress</text>\n",
       "</g>\n",
       "<!-- ln1&#45;&gt;mlp -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>ln1&#45;&gt;mlp</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M170.34,-389.43C163,-380.86 154.02,-370.38 145.53,-360.48\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"148.04,-358.03 138.87,-352.71 142.72,-362.58 148.04,-358.03\"/>\n",
       "</g>\n",
       "<!-- res2 -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>res2</title>\n",
       "<ellipse fill=\"lightgray\" stroke=\"black\" cx=\"186\" cy=\"-243.85\" rx=\"18.7\" ry=\"18.7\"/>\n",
       "<text text-anchor=\"middle\" x=\"186\" y=\"-240.15\" font-family=\"Times,serif\" font-size=\"14.00\">+</text>\n",
       "</g>\n",
       "<!-- ln1&#45;&gt;res2 -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>ln1&#45;&gt;res2</title>\n",
       "<path fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" d=\"M204.62,-389.68C213.55,-379.72 223.3,-366.58 228,-352.7 235.55,-330.38 235.56,-322.01 228,-299.7 223.61,-286.74 214.79,-274.44 206.37,-264.79\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"208.84,-262.31 199.49,-257.34 203.7,-267.06 208.84,-262.31\"/>\n",
       "<text text-anchor=\"middle\" x=\"263\" y=\"-322.5\" font-family=\"Times,serif\" font-size=\"14.00\">residual</text>\n",
       "</g>\n",
       "<!-- mlp&#45;&gt;res2 -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>mlp&#45;&gt;res2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M139.21,-299.33C148.29,-288.76 158.67,-276.67 167.35,-266.57\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"170.02,-268.83 173.88,-258.96 164.71,-264.27 170.02,-268.83\"/>\n",
       "</g>\n",
       "<!-- ln2 -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>ln2</title>\n",
       "<polygon fill=\"lightpink\" stroke=\"black\" points=\"254.5,-188 117.5,-188 117.5,-150 254.5,-150 254.5,-188\"/>\n",
       "<text text-anchor=\"middle\" x=\"186\" y=\"-172.8\" font-family=\"Times,serif\" font-size=\"14.00\">LayerNorm</text>\n",
       "<text text-anchor=\"middle\" x=\"186\" y=\"-157.8\" font-family=\"Times,serif\" font-size=\"14.00\">(sound engineer)</text>\n",
       "</g>\n",
       "<!-- res2&#45;&gt;ln2 -->\n",
       "<g id=\"edge11\" class=\"edge\">\n",
       "<title>res2&#45;&gt;ln2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M186,-224.84C186,-216.79 186,-207.1 186,-198.12\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"189.5,-198.05 186,-188.05 182.5,-198.05 189.5,-198.05\"/>\n",
       "</g>\n",
       "<!-- output -->\n",
       "<g id=\"node11\" class=\"node\">\n",
       "<title>output</title>\n",
       "<polygon fill=\"lightcoral\" stroke=\"black\" points=\"279.5,-113 92.5,-113 92.5,-75 279.5,-75 279.5,-113\"/>\n",
       "<text text-anchor=\"middle\" x=\"186\" y=\"-97.8\" font-family=\"Times,serif\" font-size=\"14.00\">New Representation</text>\n",
       "<text text-anchor=\"middle\" x=\"186\" y=\"-82.8\" font-family=\"Times,serif\" font-size=\"14.00\">(absorbed &amp; processed)</text>\n",
       "</g>\n",
       "<!-- ln2&#45;&gt;output -->\n",
       "<g id=\"edge12\" class=\"edge\">\n",
       "<title>ln2&#45;&gt;output</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M186,-149.96C186,-141.88 186,-132.18 186,-123.18\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"189.5,-123.09 186,-113.09 182.5,-123.09 189.5,-123.09\"/>\n",
       "</g>\n",
       "<!-- repeat -->\n",
       "<g id=\"node12\" class=\"node\">\n",
       "<title>repeat</title>\n",
       "<text text-anchor=\"middle\" x=\"186\" y=\"-22.8\" font-family=\"Times,serif\" font-size=\"14.00\">Ã— 12 layers</text>\n",
       "<text text-anchor=\"middle\" x=\"186\" y=\"-7.8\" font-family=\"Times,serif\" font-size=\"14.00\">(12 networking events)</text>\n",
       "</g>\n",
       "<!-- output&#45;&gt;repeat -->\n",
       "<g id=\"edge13\" class=\"edge\">\n",
       "<title>output&#45;&gt;repeat</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M186,-74.96C186,-66.88 186,-57.18 186,-48.18\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"189.5,-48.09 186,-38.09 182.5,-48.09 189.5,-48.09\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x7afcc30605f0>"
      ]
     },
     "execution_count": 0,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from graphviz import Digraph\n",
    "\n",
    "dot = Digraph(comment='Transformer Block - Complete')\n",
    "dot.attr(rankdir='TB', size='10,14')\n",
    "\n",
    "# Input\n",
    "dot.node('tok', 'Token Embedding\\n(what the word means)', shape='box', style='filled', fillcolor='lightblue')\n",
    "dot.node('pos', 'Position Embedding\\n(where it sits)', shape='box', style='filled', fillcolor='lightblue')\n",
    "dot.node('combined', 'Combined\\n(isolated, can\\'t talk yet)', shape='box', style='filled', fillcolor='lightgreen')\n",
    "\n",
    "# Transformer Block\n",
    "dot.node('block_start', 'TRANSFORMER BLOCK\\n(one networking event)', shape='box', style='filled,bold', fillcolor='white')\n",
    "\n",
    "# Attention + residual\n",
    "dot.node('mha', 'Multi-Head Attention\\n(Q/K/V dance)\\n12 heads = 12 conversations', shape='box', style='filled', fillcolor='lightyellow')\n",
    "dot.node('res1', '+', shape='circle', style='filled', fillcolor='lightgray')\n",
    "dot.node('ln1', 'LayerNorm\\n(sound engineer)', shape='box', style='filled', fillcolor='lightpink')\n",
    "\n",
    "# MLP + residual\n",
    "dot.node('mlp', 'MLP\\n(journaling)\\nexpand â†’ aha â†’ compress', shape='box', style='filled', fillcolor='lightyellow')\n",
    "dot.node('res2', '+', shape='circle', style='filled', fillcolor='lightgray')\n",
    "dot.node('ln2', 'LayerNorm\\n(sound engineer)', shape='box', style='filled', fillcolor='lightpink')\n",
    "\n",
    "# Output\n",
    "dot.node('output', 'New Representation\\n(absorbed & processed)', shape='box', style='filled', fillcolor='lightcoral')\n",
    "dot.node('repeat', 'Ã— 12 layers\\n(12 networking events)', shape='plaintext')\n",
    "\n",
    "# Edges\n",
    "dot.edge('tok', 'combined')\n",
    "dot.edge('pos', 'combined')\n",
    "dot.edge('combined', 'block_start')\n",
    "dot.edge('block_start', 'mha')\n",
    "dot.edge('mha', 'res1')\n",
    "dot.edge('block_start', 'res1', style='dashed', label='residual\\n(don\\'t forget)')\n",
    "dot.edge('res1', 'ln1')\n",
    "dot.edge('ln1', 'mlp')\n",
    "dot.edge('mlp', 'res2')\n",
    "dot.edge('ln1', 'res2', style='dashed', label='residual')\n",
    "dot.edge('res2', 'ln2')\n",
    "dot.edge('ln2', 'output')\n",
    "dot.edge('output', 'repeat')\n",
    "\n",
    "dot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31fe0dfe",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸŽ¯ TL;DR Summary\n",
    "\n",
    "A transformer is like attending **12 networking parties** (layers):\n",
    "\n",
    "1. **Arrive** with your ID card (token + position embedding)\n",
    "2. **Dance** with everyone (multi-head attention with Q/K/V)\n",
    "3. **Absorb** knowledge from relevant people (attention weights Ã— V)\n",
    "4. **Balance** your energy (LayerNorm)\n",
    "5. **Journal** at home (MLP: expand â†’ aha â†’ compress)\n",
    "6. **Don't forget** who you are (residual connections)\n",
    "7. **Repeat** 12 times for deep understanding\n",
    "\n",
    "After 12 parties, you're a networking pro with rich, layered understanding of the world! ðŸŽ‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa5b050",
   "metadata": {
    "time_run": "2025-12-31T14:55:03.535929+00:00"
   },
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "from dialoghelper import curr_dialog\n",
    "\n",
    "def deploy_notebook():\n",
    "    nb_name = Path(curr_dialog()['name']).name + '.ipynb'\n",
    "    src = f'/app/data/{curr_dialog()[\"name\"]}.ipynb'\n",
    "    dst = '/app/data/publish/portfolio/static/'\n",
    "    print(nb_name)\n",
    "    \n",
    "    # Copy notebook to static folder\n",
    "    subprocess.run(['cp', src, dst])\n",
    "    \n",
    "    # Deploy with plash\n",
    "    subprocess.run(['plash_deploy'], cwd='/app/data/publish/portfolio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec0fb4c",
   "metadata": {
    "time_run": "2025-12-31T14:55:03.542502+00:00"
   },
   "outputs": [],
   "source": [
    "#| hide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668b6e85",
   "metadata": {
    "time_run": "2025-12-31T14:55:03.548409+00:00"
   },
   "outputs": [],
   "source": [
    "#| hide\n",
    "deploy_notebook()"
   ]
  }
 ],
 "metadata": {
  "solveit_dialog_mode": "learning",
  "solveit_ver": 2
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
