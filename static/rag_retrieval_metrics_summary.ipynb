{
 "cells": [
  {
   "cell_type": "raw",
   "id": "5990f468",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Understanding Retrieval Metrics for RAG Systems\"\n",
    "author: \"Gaurav Adlakha\"\n",
    "date: \"2025-12-21\"\n",
    "categories: [RAG, ML, Evaluation Metrics]\n",
    "description: \"An overview of retrieval metrics (MRR, MAP, Precision, Recall) and techniques like RRF for building effective RAG systems.\"\n",
    "toc: true\n",
    "toc-depth: 2\n",
    "format:\n",
    "  html:\n",
    "    code-fold: show\n",
    "    code-tools: true\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa9e059",
   "metadata": {},
   "source": [
    "# RAG, Retrieval & Evaluation Metrics â€” Complete Study Notes\n",
    "\n",
    "A comprehensive guide covering contrastive learning, embeddings, hybrid search fusion, and retrieval evaluation metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6188081",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Contrastive Learning & Embeddings\n",
    "\n",
    "### What is Contrastive Learning?\n",
    "\n",
    "A technique where a model learns by **comparing examples** â€” pulling similar things closer together and pushing dissimilar things apart in a learned vector space.\n",
    "\n",
    "### The Core Idea\n",
    "\n",
    "- **Anchor**: The thing you're looking at\n",
    "- **Positive**: Something similar to the anchor\n",
    "- **Negative**: Something different from the anchor\n",
    "\n",
    "**Goal**: Learn representations where:\n",
    "- `distance(anchor, positive)` â†’ **small**\n",
    "- `distance(anchor, negative)` â†’ **large**\n",
    "\n",
    "### The Basic Algorithm (Triplet Loss)\n",
    "\n",
    "```\n",
    "loss = max(0, distance(anchor, positive) - distance(anchor, negative) + margin)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36215964",
   "metadata": {},
   "source": [
    "### How Do We Know What's Similar? (No Manual Labels Needed!)\n",
    "\n",
    "The clever trick is **self-supervision** â€” creating similarity automatically from data structure:\n",
    "\n",
    "| Strategy | How it works |\n",
    "|----------|-------------|\n",
    "| **Data augmentation (SimCSE)** | Same sentence + different dropout = positive pair |\n",
    "| **Nearby context (Word2Vec, BERT)** | Words/sentences close in document = similar |\n",
    "| **Back-translation** | Original + translated-back version = positive pair |\n",
    "| **Natural pairs** | Question-answer, title-body, query-clicked result |\n",
    "\n",
    "### How Off-the-Shelf Embeddings (OpenAI, etc.) Work So Well\n",
    "\n",
    "They train on **massive naturally occurring paired data**:\n",
    "- Web pages and their titles\n",
    "- Reddit posts and top comments\n",
    "- Wikipedia sections and headings\n",
    "- Forum questions and accepted answers\n",
    "\n",
    "> **Key Insight**: Humans already created millions of implicit pairs by how they structured content online â€” no manual labeling needed!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be62f2d1",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Reciprocal Rank Fusion (RRF)\n",
    "\n",
    "### The Problem\n",
    "\n",
    "How do you combine rankings from multiple search systems (e.g., semantic search + keyword search)?\n",
    "\n",
    "### The Formula\n",
    "\n",
    "$$\\text{RRF}(d) = \\sum_{\\text{each ranker}} \\frac{1}{k + \\text{rank}(d)}$$\n",
    "\n",
    "Where **k** is a constant (usually 60)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dedd338",
   "metadata": {},
   "source": [
    "### Sticky Analogy: The Restaurant Recommendations ðŸ½ï¸\n",
    "\n",
    "> **Imagine asking multiple friends for restaurant recommendations.** Instead of just counting votes, give more credit to items ranked higher â€” but being consistently \"pretty good\" across all friends beats being one person's favorite but ignored by others."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56b4158",
   "metadata": {},
   "source": [
    "### Numerical Example\n",
    "\n",
    "Two search systems ranking documents (k=60):\n",
    "\n",
    "| Document | Semantic Rank | Keyword Rank | RRF Score |\n",
    "|----------|---------------|--------------|----------|\n",
    "| Doc A    | 1             | 5            | 1/61 + 1/65 = **0.0318** |\n",
    "| Doc B    | 3             | 1            | 1/63 + 1/61 = **0.0323** |\n",
    "| Doc C    | 2             | 2            | 1/62 + 1/62 = **0.0322** |\n",
    "\n",
    "**Final RRF Ranking: B â†’ C â†’ A**\n",
    "\n",
    "> **Key insight**: Doc C (ranked 2nd, 2nd) beats Doc A (ranked 1st, 5th) because **RRF rewards consistency**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d52fd2",
   "metadata": {},
   "source": [
    "### The k Parameter: The \"Patience\" Knob ðŸŽ›ï¸\n",
    "\n",
    "| k value | Behavior | Analogy |\n",
    "|---------|----------|--------|\n",
    "| **Low k (e.g., 1)** | Top ranks dominate heavily | Impatient judge â€” only looks at gold medalists |\n",
    "| **High k (e.g., 60)** | Differences compressed, rewards consistency | Patient judge â€” \"top 10 are all pretty good\" |\n",
    "\n",
    "#### Numerical Proof (from our exploration):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1effc011",
   "metadata": {
    "time_run": "2025-12-21T15:42:26.643698+00:00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without k: gap = 0.9\n",
      "With k=50: gap = 0.002941\n",
      "Gap shrinks to 0.33% of original!\n"
     ]
    }
   ],
   "source": [
    "# Without k (k=0): Gap between rank 1 and rank 10\n",
    "gap_no_k = (1/1) - (1/10)\n",
    "print(f\"Without k: gap = {gap_no_k}\")\n",
    "\n",
    "# With k=50: Gap between rank 1 and rank 10\n",
    "gap_with_k = (1/51) - (1/60)\n",
    "print(f\"With k=50: gap = {gap_with_k:.6f}\")\n",
    "\n",
    "# Compression ratio\n",
    "print(f\"Gap shrinks to {gap_with_k/gap_no_k:.2%} of original!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6592d1",
   "metadata": {},
   "source": [
    "> **Insight**: Adding k compresses the differences between ranks â€” making #1 vs #10 feel almost the same, instead of 10x better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6d082e",
   "metadata": {},
   "source": [
    "### The Î² (Beta) Parameter: Weighting Rankers\n",
    "\n",
    "When you want to trust one ranker more than another:\n",
    "\n",
    "```\n",
    "RRF(d) = Î² Ã— 1/(k + semantic_rank) + (1-Î²) Ã— 1/(k + keyword_rank)\n",
    "```\n",
    "\n",
    "| Î² value | Effect |\n",
    "|---------|--------|\n",
    "| Î² = 1.0 | Pure semantic search |\n",
    "| Î² = 0.5 | Equal weight |\n",
    "| Î² = 0.8 | 80% semantic, 20% keyword |\n",
    "| Î² = 0.0 | Pure keyword search |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af01807",
   "metadata": {},
   "source": [
    "### RRF Protects Against Bad Rankers\n",
    "\n",
    "> **Panel of Judges Analogy**: If 4 judges are fair and 1 is bribed to push Contestant Z, that one loud wrong voice loses to four quieter right voices. The irrelevant item gets boosted by one ranker, but can't overcome consensus from others.\n",
    "\n",
    "**Benefits of RRF:**\n",
    "- Simple â€” no training needed\n",
    "- Robust to bad rankers\n",
    "- Works well for hybrid search\n",
    "- Just combine your lists and go!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1e3608",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Precision vs Recall Tradeoff\n",
    "\n",
    "### The Fundamental Tradeoff\n",
    "\n",
    "| Action | Precision | Recall |\n",
    "|--------|-----------|--------|\n",
    "| Retrieve fewer docs | âœ… High | âŒ Low (miss relevant items) |\n",
    "| Retrieve more docs | âŒ Low (more noise) | âœ… High |\n",
    "\n",
    "### Example\n",
    "\n",
    "| Retrieval | Precision | Recall |\n",
    "|-----------|-----------|--------|\n",
    "| 12 docs retrieved | 66% (8/12) | 80% (8/10) |\n",
    "| 15 docs retrieved | 60% (9/15) | 90% (9/10) |\n",
    "\n",
    "> **How RRF helps**: Retrieve more from each ranker (boost recall), but fusion acts as a quality filter â€” only docs appearing in multiple rankings score high. Maintains precision even with larger retrieval sets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3b22b0",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Search Evaluation Metrics\n",
    "\n",
    "### Overview Table\n",
    "\n",
    "| Metric | What it measures | Formula | Sticky Analogy |\n",
    "|--------|------------------|---------|----------------|\n",
    "| **MRR** | Position of first correct answer | avg(1/rank) | Waiter bringing your order on 1st vs 3rd try |\n",
    "| **Precision@K** | Quality of top K results | relevant in K / K | â€” |\n",
    "| **Recall@K** | Coverage of relevant items | found / total relevant | â€” |\n",
    "| **MAP** | Ranking quality of all relevant items | avg precision at each relevant doc | ðŸ›’ Shopping list efficiency |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c9fa48",
   "metadata": {},
   "source": [
    "### Mean Reciprocal Rank (MRR)\n",
    "\n",
    "**Formula**: Average of `1/rank` for the first correct answer across queries.\n",
    "\n",
    "| First relevant at rank | Score |\n",
    "|------------------------|-------|\n",
    "| 1 | 1.0 |\n",
    "| 2 | 0.5 |\n",
    "| 4 | 0.25 |\n",
    "\n",
    "> **Analogy**: Rating restaurants by how quickly the waiter brings what you actually ordered. First try = perfect. Third try = frustrating.\n",
    "\n",
    "**Example calculation**:\n",
    "- Query 1: first relevant at rank 2 â†’ 1/2 = 0.5\n",
    "- Query 2: first relevant at rank 1 â†’ 1/1 = 1.0\n",
    "- Query 3: first relevant at rank 5 â†’ 1/5 = 0.2\n",
    "\n",
    "**MRR = (0.5 + 1.0 + 0.2) / 3 = 0.567**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac5f306",
   "metadata": {},
   "source": [
    "### Mean Average Precision (MAP)\n",
    "\n",
    "**Formula**: $$AP = \\frac{1}{R} \\sum_{k: \\text{doc}_k \\text{ is relevant}} \\text{Precision}@k$$\n",
    "\n",
    "### Sticky Analogy: The Shopping Trip ðŸ›’\n",
    "\n",
    "> **\"How little did I wander before finding what I needed?\"**\n",
    ">\n",
    "> You're in a supermarket with a list of 3 items. Each time you find a list item, ask yourself: \"How efficient have I been so far?\" (items on list Ã· total grabbed). Average those efficiency checks = your MAP score.\n",
    "\n",
    "**Example**:\n",
    "- Find list item at pick 1: 1/1 = 100%\n",
    "- Find list item at pick 4: 2/4 = 50%\n",
    "- Find list item at pick 5: 3/5 = 60%\n",
    "\n",
    "**AP = (100% + 50% + 60%) / 3 = 70%**\n",
    "\n",
    "> **Key insight**: MAP rewards finding relevant items *early* â€” less wandering = higher score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c16c6f",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Choosing the Right Metric for Your Use Case\n",
    "\n",
    "### Decision Guide\n",
    "\n",
    "| Your Priority | Metric to Use | Example Use Case |\n",
    "|---------------|---------------|------------------|\n",
    "| First answer must be right | **MRR** | Customer support chatbot |\n",
    "| Don't miss any relevant docs | **Recall@K** | Legal research |\n",
    "| Minimize noise/irrelevant results | **Precision@K** | E-commerce filters |\n",
    "| Rank good stuff early | **MAP** | Search results browsing |\n",
    "\n",
    "> **Key insight**: There's no universally \"best\" metric â€” the right one depends on whether your users value **speed**, **completeness**, or **cleanliness** of results.\n",
    "\n",
    "### Interpreting Metric Changes\n",
    "\n",
    "**Scenario**: After tuning RRF, MRR went up but Recall dropped.\n",
    "\n",
    "- **Good if**: Building a chatbot (first answer matters most)\n",
    "- **Bad if**: Building legal search (need ALL relevant docs)\n",
    "\n",
    "**Scenario**: High Recall@20 but low MAP\n",
    "\n",
    "- **Diagnosis**: Finding relevant docs, but ranking them poorly (appearing late instead of early)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0701f8",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Quick Reference\n",
    "\n",
    "### RRF Cheat Sheet\n",
    "\n",
    "```\n",
    "RRF(d) = Î£ 1/(k + rank)\n",
    "```\n",
    "\n",
    "- **k=60** (default): Balanced, rewards consistency\n",
    "- **Low k**: Trust top picks strongly\n",
    "- **High k**: Compress rank differences\n",
    "- **Î²**: Weight between rankers (0-1)\n",
    "\n",
    "### Metrics Cheat Sheet\n",
    "\n",
    "| Metric | One-liner |\n",
    "|--------|----------|\n",
    "| **MRR** | \"How far do I scroll to find the first right answer?\" |\n",
    "| **Precision@K** | \"How much of my top K is good?\" |\n",
    "| **Recall@K** | \"How much of the good stuff did I find?\" |\n",
    "| **MAP** | \"How little did I wander before finding what I needed?\" |\n",
    "\n",
    "### All Metrics Require Ground Truth\n",
    "\n",
    "You need to know which documents are actually relevant to evaluate any of these metrics!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fffe15a4",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Quick Answers\n",
    "\n",
    "**Q: Explain contrastive learning in one sentence.**\n",
    "> Train a model to pull similar items close and push different items apart in vector space, using natural pairs from data structure instead of manual labels.\n",
    "\n",
    "**Q: What is RRF and when would you use it?**\n",
    "> A simple way to combine rankings from multiple search systems by summing 1/(k+rank). Use it for hybrid search (semantic + keyword) â€” no training needed, robust to bad rankers.\n",
    "\n",
    "**Q: What's the difference between Precision and Recall?**\n",
    "> Precision = \"of what I returned, how much is good?\" Recall = \"of all the good stuff, how much did I find?\" Tradeoff: more results â†’ higher recall, lower precision.\n",
    "\n",
    "**Q: When would you use MRR vs MAP?**\n",
    "> MRR when only the first result matters (chatbots). MAP when you care about ranking quality across all relevant items (search browsing).\n",
    "\n",
    "**Q: How does the k parameter in RRF work?**\n",
    "> It's a \"patience\" knob. Low k = trust top picks strongly. High k = compress differences, reward consistency across rankers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50ea683",
   "metadata": {
    "time_run": "2025-12-21T15:47:01.925761+00:00"
   },
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "from dialoghelper import curr_dialog\n",
    "\n",
    "def deploy_notebook():\n",
    "    nb_name = Path(curr_dialog()['name']).name + '.ipynb'\n",
    "    src = f'/app/data/{curr_dialog()[\"name\"]}.ipynb'\n",
    "    dst = '/app/data/publish/portfolio/static/'\n",
    "    print(nb_name)\n",
    "    \n",
    "    # Copy notebook to static folder\n",
    "    subprocess.run(['cp', src, dst])\n",
    "    \n",
    "    # Deploy with plash\n",
    "    subprocess.run(['plash_deploy'], cwd='/app/data/publish/portfolio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d2d395",
   "metadata": {
    "time_run": "2025-12-21T16:06:37.663176+00:00"
   },
   "outputs": [],
   "source": [
    "#| hide\n",
    "deploy_notebook()"
   ]
  }
 ],
 "metadata": {
  "solveit_dialog_mode": "learning",
  "solveit_ver": 2
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
