{
 "cells": [
  {
   "cell_type": "raw",
   "id": "992ac507",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Getting Started with SageMaker Serverless Endpoints\"\n",
    "author: \"Gaurav Adlakha\"\n",
    "date: \"December 19, 2025\"\n",
    "categories:\n",
    "  - AWS\n",
    "  - SageMaker\n",
    "  - Machine Learning\n",
    "  - MLOps\n",
    "  - HuggingFace\n",
    "description: \"A practical guide to deploying serverless ML inference endpoints on SageMaker, from setup to cleanup, with zero cost for testing.\"\n",
    "toc: true\n",
    "toc-depth: 2\n",
    "format:\n",
    "  html:\n",
    "    code-fold: true\n",
    "    code-tools: true\n",
    "    highlight-style: github\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db3e720",
   "metadata": {
    "hidden": true
   },
   "source": [
    "# üß† SageMaker Serverless Exploration - Complete Summary\n",
    "**Total Cost:** $0.00 üéâ  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6c5907",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## üìã Table of Contents\n",
    "\n",
    "1. [Architecture Overview](#architecture-overview)\n",
    "2. [IAM Role Setup](#iam-role-setup)\n",
    "3. [SageMaker SDK Setup](#sagemaker-sdk-setup)\n",
    "4. [Deploying a Serverless Endpoint](#deploying-serverless-endpoint)\n",
    "5. [Testing the Endpoint](#testing-the-endpoint)\n",
    "6. [Cleanup & Cost Management](#cleanup-cost-management)\n",
    "7. [Production Workflows](#production-workflows)\n",
    "8. [High-Performance Options](#high-performance-options)\n",
    "9. [CPU vs GPU Selection](#cpu-vs-gpu-selection)\n",
    "10. [Quick Reference](#quick-reference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197fb22f",
   "metadata": {
    "hidden": true
   },
   "source": [
    "---\n",
    "## üèóÔ∏è Architecture Overview <a name=\"architecture-overview\"></a>\n",
    "\n",
    "### What We Built\n",
    "\n",
    "```\n",
    "IAM Role ‚Üí HuggingFace Model ‚Üí Serverless Endpoint ‚Üí Inference Call ‚Üí Cleanup\n",
    "```\n",
    "\n",
    "### AWS Services Used\n",
    "\n",
    "| Service | Purpose | Cost |\n",
    "|---------|---------|------|\n",
    "| **IAM** | Permission management | Free |\n",
    "| **SageMaker** | ML model hosting | Pay per inference |\n",
    "| **HuggingFace Hub** | Pre-trained model source | Free |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afafcf8d",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> üí° **Sticky Analogy: Food Truck Service**\n",
    ">\n",
    "> Think of SageMaker Serverless as ordering a **food truck on-demand**:\n",
    "> - **IAM Role** = Your ID badge proving you're allowed to order\n",
    "> - **HuggingFaceModel** = The menu item you're ordering\n",
    "> - **ServerlessInferenceConfig** = Delivery preferences (memory, concurrency)\n",
    "> - **model.deploy()** = Actually placing the order\n",
    "> - **Endpoint** = The food truck arrives and flips the \"OPEN\" sign"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bea8178",
   "metadata": {
    "hidden": true
   },
   "source": [
    "---\n",
    "## üîê IAM Role Setup <a name=\"iam-role-setup\"></a>\n",
    "\n",
    "### Why We Need It\n",
    "\n",
    "SageMaker needs permission to access S3, ECR, and other AWS services on your behalf.\n",
    "\n",
    "> üí° **Analogy:** Like giving a delivery driver your house key to drop off packages while you're away.\n",
    "\n",
    "### What We Created\n",
    "\n",
    "- **Role Name:** `SageMakerExecutionRole`\n",
    "- **ARN:** `arn:aws:iam::609662024349:role/SageMakerExecutionRole`\n",
    "- **Trust Policy:** Allows `sagemaker.amazonaws.com` to assume the role\n",
    "- **Permission Policy:** `AmazonSageMakerFullAccess`\n",
    "\n",
    "### Trust Policy JSON\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"Version\": \"2012-10-17\",\n",
    "  \"Statement\": [\n",
    "    {\n",
    "      \"Effect\": \"Allow\",\n",
    "      \"Principal\": {\n",
    "        \"Service\": \"sagemaker.amazonaws.com\"\n",
    "      },\n",
    "      \"Action\": \"sts:AssumeRole\"\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b81cb73",
   "metadata": {
    "hidden": true
   },
   "source": [
    "---\n",
    "## üì¶ SageMaker SDK Setup <a name=\"sagemaker-sdk-setup\"></a>\n",
    "\n",
    "### Version Compatibility Issue\n",
    "\n",
    "**Problem:** `ModuleNotFoundError: No module named 'sagemaker.huggingface'`\n",
    "\n",
    "**Root Cause:** SageMaker v3.x restructured modules - HuggingFace integration was removed/moved.\n",
    "\n",
    "> üí° **Analogy: App Store Update**\n",
    ">\n",
    "> Like buying a new iPhone and finding your favorite app hasn't been updated for the new iOS yet. Rolling back to v2 is like using the \"classic\" version that still has everything built-in.\n",
    "\n",
    "### Solution\n",
    "\n",
    "```bash\n",
    "pip3 install \"sagemaker>=2.0,<3.0\"\n",
    "```\n",
    "\n",
    "### Version Comparison\n",
    "\n",
    "| Version | HuggingFaceModel | Notes |\n",
    "|---------|------------------|-------|\n",
    "| **v3.x** | ‚ùå Not bundled | Modular architecture |\n",
    "| **v2.x** | ‚úÖ Included | Use this for HuggingFace |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed50467",
   "metadata": {
    "hidden": true
   },
   "source": [
    "---\n",
    "## üöÄ Deploying a Serverless Endpoint <a name=\"deploying-serverless-endpoint\"></a>\n",
    "\n",
    "### Initial Approach (Failed)\n",
    "\n",
    "Using S3 path directly:\n",
    "\n",
    "```python\n",
    "model = HuggingFaceModel(\n",
    "    model_data=\"s3://huggingface-sagemaker-models/...\",  # ‚ùå Access denied\n",
    "    ...\n",
    ")\n",
    "```\n",
    "\n",
    "**Error:** `ValidationException: Could not access model data at s3://...`\n",
    "\n",
    "> üí° **Analogy: Supplier vs Warehouse**\n",
    ">\n",
    "> Instead of giving the delivery truck a specific warehouse address that might be outdated, tell them \"order directly from the supplier\" (HuggingFace Hub) - always fresh and accessible!\n",
    "\n",
    "### Working Solution\n",
    "\n",
    "Using HuggingFace Hub directly via environment variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80b5dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sagemaker-test.py - Working deployment script\n",
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "from sagemaker.serverless import ServerlessInferenceConfig\n",
    "\n",
    "role = \"arn:aws:iam::609662024349:role/SageMakerExecutionRole\"\n",
    "\n",
    "# Use HuggingFace Hub directly instead of S3\n",
    "model = HuggingFaceModel(\n",
    "    transformers_version=\"4.26\",\n",
    "    pytorch_version=\"1.13\",\n",
    "    py_version=\"py39\",\n",
    "    role=role,\n",
    "    env={\"HF_MODEL_ID\": \"distilbert-base-uncased-finetuned-sst-2-english\"}\n",
    ")\n",
    "\n",
    "serverless_config = ServerlessInferenceConfig(\n",
    "    memory_size_in_mb=2048,\n",
    "    max_concurrency=1\n",
    ")\n",
    "\n",
    "predictor = model.deploy(serverless_inference_config=serverless_config)\n",
    "print(f\"Endpoint name: {predictor.endpoint_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0bd0e6f",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Deployment Progress\n",
    "\n",
    "```\n",
    "----!\n",
    "Endpoint name: huggingface-pytorch-inference-2025-12-23-13-07-31-668\n",
    "```\n",
    "\n",
    "**What the symbols mean:**\n",
    "- Each `-` = Health check in progress\n",
    "- `!` = Endpoint is ready!\n",
    "\n",
    "> üí° **Analogy:** The food truck is driving to the location, setting up the kitchen, firing up the grill, and flipping the \"OPEN\" sign."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf84c727",
   "metadata": {
    "hidden": true
   },
   "source": [
    "---\n",
    "## üß™ Testing the Endpoint <a name=\"testing-the-endpoint\"></a>\n",
    "\n",
    "### Shell Quoting Lesson Learned\n",
    "\n",
    "**Problem:** Inline Python via SSH causes quote escaping nightmares.\n",
    "\n",
    "> üí° **Analogy: Noisy Drive-Through**\n",
    ">\n",
    "> Instead of shouting a complicated order through a noisy speaker (nested shell quotes), write it on paper first (file), then hand it through the window!\n",
    "\n",
    "### Solution: File Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19e3c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test-endpoint.py - Inference script\n",
    "import boto3\n",
    "import json\n",
    "\n",
    "runtime = boto3.client(\"sagemaker-runtime\", region_name=\"us-east-1\")\n",
    "\n",
    "response = runtime.invoke_endpoint(\n",
    "    EndpointName=\"huggingface-pytorch-inference-2025-12-23-13-07-31-668\",\n",
    "    ContentType=\"application/json\",\n",
    "    Body=json.dumps({\"inputs\": \"I love learning AWS!\"})\n",
    ")\n",
    "\n",
    "print(json.loads(response[\"Body\"].read().decode()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896ac99a",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Test Results\n",
    "\n",
    "| Input | Label | Score |\n",
    "|-------|-------|-------|\n",
    "| \"I love learning AWS!\" | POSITIVE | 99.95% |\n",
    "\n",
    "```json\n",
    "[{\"label\": \"POSITIVE\", \"score\": 0.9995132684707642}]\n",
    "```\n",
    "\n",
    "> üí° **The model is like a mood detector** - it reads the emotional tone of text and tells you whether it's positive or negative, with a confidence percentage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9850f2d7",
   "metadata": {
    "hidden": true
   },
   "source": [
    "---\n",
    "## üßπ Cleanup & Cost Management <a name=\"cleanup-cost-management\"></a>\n",
    "\n",
    "### Why Cleanup Matters\n",
    "\n",
    "> üí° **Analogy: Closing the Food Truck**\n",
    ">\n",
    "> The endpoint is like a food truck parked with the \"OPEN\" sign on. Even if no customers come, there's a small cost for being ready to serve. Deleting = packing up and leaving!\n",
    "\n",
    "### Cleanup Commands\n",
    "\n",
    "```bash\n",
    "# 1. Delete endpoint (stops billing)\n",
    "aws sagemaker delete-endpoint --endpoint-name <endpoint-name>\n",
    "\n",
    "# 2. Delete endpoint config (free, but keeps things clean)\n",
    "aws sagemaker delete-endpoint-config --endpoint-config-name <config-name>\n",
    "\n",
    "# 3. Delete model (free, but keeps things clean)\n",
    "aws sagemaker delete-model --model-name <model-name>\n",
    "\n",
    "# 4. Verify everything is gone\n",
    "aws sagemaker list-endpoints           # Should be empty\n",
    "aws sagemaker list-endpoint-configs    # Should be empty\n",
    "aws sagemaker list-models              # Should be empty\n",
    "```\n",
    "\n",
    "### What Are These Resources?\n",
    "\n",
    "| Resource | What It Is | Cost | Analogy |\n",
    "|----------|------------|------|--------|\n",
    "| **Endpoint** | Running inference service | üí∞ Charges | The food truck serving |\n",
    "| **Endpoint Config** | Blueprint for endpoint setup | Free | Recipe card |\n",
    "| **Model** | Registration record pointing to model | Free | Catalog entry |\n",
    "\n",
    "### AWS Billing Note\n",
    "\n",
    "AWS billing has a **6-24 hour delay**. Charges may not appear immediately, but for a few test calls, expect **fractions of a penny**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7330ebc",
   "metadata": {
    "hidden": true
   },
   "source": [
    "---\n",
    "## üè≠ Production Workflows <a name=\"production-workflows\"></a>\n",
    "\n",
    "### Learning vs Production\n",
    "\n",
    "| Stage | Source | Code |\n",
    "|-------|--------|------|\n",
    "| **Learning** | HuggingFace Hub | `env={\"HF_MODEL_ID\": \"...\"}` |\n",
    "| **Production** | Your S3 bucket | `model_data=\"s3://your-bucket/model.tar.gz\"` |\n",
    "\n",
    "### Production Flow\n",
    "\n",
    "```\n",
    "Train model locally/SageMaker\n",
    "        ‚Üì\n",
    "Save/export model (model.tar.gz)\n",
    "        ‚Üì\n",
    "Upload to YOUR S3 bucket\n",
    "        ‚Üì\n",
    "Deploy from S3\n",
    "```\n",
    "\n",
    "> üí° **Analogy: Restaurant vs Home Cooking**\n",
    ">\n",
    "> - **Today:** Ordered pre-made dish from restaurant (HuggingFace Hub)\n",
    "> - **Production:** Cook your own recipe, package it, store in your pantry (S3), serve from there"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c5bc4a",
   "metadata": {
    "hidden": true
   },
   "source": [
    "---\n",
    "## üöÄ High-Performance Options <a name=\"high-performance-options\"></a>\n",
    "\n",
    "### Endpoint Type Comparison\n",
    "\n",
    "| Type | Behavior | Best For |\n",
    "|------|----------|----------|\n",
    "| **Serverless** | Spins up on-demand, scales to zero | Low traffic, cost-sensitive, dev/test |\n",
    "| **Real-time** | Instance runs 24/7 | High throughput, low latency, production |\n",
    "| **Async** | Queue-based, for long jobs | Large payloads, batch processing |\n",
    "\n",
    "### Serverless vs Real-time Trade-offs\n",
    "\n",
    "| | Serverless | Real-time |\n",
    "|--|-----------|----------|\n",
    "| **Cold start** | 10-30 sec first call | None (always warm) |\n",
    "| **Latency** | Higher | Lower (~ms) |\n",
    "| **Cost when idle** | $0 | Paying 24/7 |\n",
    "| **High traffic** | ‚ùå | ‚úÖ |\n",
    "\n",
    "> üí° **Analogy:**\n",
    "> - **Serverless** = Food truck that parks only when you call (cheap but slow to arrive)\n",
    "> - **Real-time** = Restaurant that's always open (instant service but paying rent 24/7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3033024",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### High Throughput + Low Latency Solution\n",
    "\n",
    "**Real-time Endpoints with Auto-Scaling**\n",
    "\n",
    "> üí° **Analogy: Fleet of Food Trucks**\n",
    ">\n",
    "> Instead of one truck that shows up when called (serverless), you have a **fleet** that automatically dispatches more trucks during lunch rush and sends them home when quiet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d693eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Deploy real-time (not serverless)\n",
    "predictor = model.deploy(\n",
    "    initial_instance_count=2,      # Start with 2 instances\n",
    "    instance_type=\"ml.m5.large\"    # Always-on instance type\n",
    ")\n",
    "\n",
    "# 2. Add auto-scaling\n",
    "import boto3\n",
    "\n",
    "client = boto3.client(\"application-autoscaling\")\n",
    "\n",
    "# Register scalable target\n",
    "client.register_scalable_target(\n",
    "    ServiceNamespace=\"sagemaker\",\n",
    "    ResourceId=f\"endpoint/{endpoint_name}/variant/AllTraffic\",\n",
    "    ScalableDimension=\"sagemaker:variant:DesiredInstanceCount\",\n",
    "    MinCapacity=2,\n",
    "    MaxCapacity=10\n",
    ")\n",
    "\n",
    "# Add scaling policy (scale based on invocations)\n",
    "client.put_scaling_policy(\n",
    "    PolicyName=\"scale-on-invocations\",\n",
    "    ServiceNamespace=\"sagemaker\",\n",
    "    ResourceId=f\"endpoint/{endpoint_name}/variant/AllTraffic\",\n",
    "    ScalableDimension=\"sagemaker:variant:DesiredInstanceCount\",\n",
    "    PolicyType=\"TargetTrackingScaling\",\n",
    "    TargetTrackingScalingPolicyConfiguration={\n",
    "        \"TargetValue\": 70.0,\n",
    "        \"PredefinedMetricSpecification\": {\n",
    "            \"PredefinedMetricType\": \"SageMakerVariantInvocationsPerInstance\"\n",
    "        },\n",
    "        \"ScaleOutCooldown\": 60,\n",
    "        \"ScaleInCooldown\": 300\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505672ad",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### High-Performance Checklist üìã\n",
    "\n",
    "When asked \"How do you handle throughput & latency in SageMaker?\", know these:\n",
    "\n",
    "| Concept | Remember It As... |\n",
    "|---------|-------------------|\n",
    "| **Endpoint Type** | How eager is your service? (Always ready / Wake on call / Queue it) |\n",
    "| **Instance Selection** | Brains (CPU) vs Muscle (GPU) - match worker to job |\n",
    "| **Scaling Strategy** | When to hire/fire more workers |\n",
    "| **Cooldown Periods** | Don't panic-hire or panic-fire |\n",
    "| **Model Optimization** | Make the model faster, not just more hardware |\n",
    "\n",
    "> üí° **Analogy: Restaurant Staffing**\n",
    ">\n",
    "> Running a high-performance ML service is like managing a restaurant - decide if you're 24/7 or pop-up (endpoint), hire cooks vs dishwashers (instance), know when to call in extra staff (scaling), don't overreact to one busy hour (cooldown), and train your staff to work faster (optimization)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a1cdfd",
   "metadata": {
    "hidden": true
   },
   "source": [
    "---\n",
    "## üß† CPU vs GPU Selection <a name=\"cpu-vs-gpu-selection\"></a>\n",
    "\n",
    "### The Confusion\n",
    "\n",
    "\"Why do we need GPU for inference? I thought GPU was only for training.\"\n",
    "\n",
    "### The Answer\n",
    "\n",
    "It depends on **model size and throughput**, not just training vs inference.\n",
    "\n",
    "| Scenario | CPU | GPU |\n",
    "|----------|-----|-----|\n",
    "| Training | ‚ùå (too slow) | ‚úÖ Always |\n",
    "| Inference - Small model | ‚úÖ | Overkill |\n",
    "| Inference - Large model (BERT, GPT) | ‚ùå (too slow) | ‚úÖ |\n",
    "| Inference - High batch volume | ‚ùå | ‚úÖ |\n",
    "\n",
    "### Quick Decision Guide\n",
    "\n",
    "| Use CPU | Use GPU |\n",
    "|---------|---------|\n",
    "| Traditional ML (XGBoost, RF) | Deep Learning (Transformers, CNNs) |\n",
    "| Small models | Large models (100M+ params) |\n",
    "| Low inference volume | High batch throughput |\n",
    "| Cost-sensitive | Latency-critical |\n",
    "\n",
    "**Simple Rule:** If it's a neural network AND (large OR fast) ‚Üí GPU\n",
    "\n",
    "> üí° **Analogy: Pizza Kitchen**\n",
    ">\n",
    "> Even after you've **learned** to cook (training), making **100 pizzas at once** (inference) still needs industrial ovens (GPU). But making **one sandwich**? A regular kitchen (CPU) works fine!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14347a49",
   "metadata": {
    "hidden": true
   },
   "source": [
    "---\n",
    "## üìö Quick Reference <a name=\"quick-reference\"></a>\n",
    "\n",
    "### Complete Command Sequence\n",
    "\n",
    "```bash\n",
    "# 1. Install SDK (use v2 for HuggingFace)\n",
    "pip3 install \"sagemaker>=2.0,<3.0\"\n",
    "\n",
    "# 2. Deploy (see Python script above)\n",
    "python3 sagemaker-test.py\n",
    "\n",
    "# 3. Test\n",
    "python3 test-endpoint.py\n",
    "\n",
    "# 4. Cleanup\n",
    "aws sagemaker delete-endpoint --endpoint-name <name>\n",
    "aws sagemaker delete-endpoint-config --endpoint-config-name <name>\n",
    "aws sagemaker delete-model --model-name <name>\n",
    "\n",
    "# 5. Verify\n",
    "aws sagemaker list-endpoints\n",
    "aws sagemaker list-endpoint-configs\n",
    "aws sagemaker list-models\n",
    "```\n",
    "\n",
    "### All Sticky Analogies\n",
    "\n",
    "| Concept | Analogy |\n",
    "|---------|--------|\n",
    "| SageMaker Serverless | Food truck that arrives on-demand |\n",
    "| IAM Role | ID badge / house key for delivery driver |\n",
    "| HF Hub vs S3 | Ordering from supplier vs specific warehouse |\n",
    "| SDK v3 vs v2 | New iPhone missing your favorite app |\n",
    "| Shell quoting | Noisy drive-through vs written order |\n",
    "| Endpoint deletion | Closing the food truck |\n",
    "| Real-time + Auto-scaling | Fleet of food trucks |\n",
    "| CPU vs GPU | Brains vs Muscle / Regular vs Industrial kitchen |\n",
    "| Cooldown periods | Don't panic-hire or panic-fire |\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Always use SageMaker SDK v2.x** for HuggingFace models\n",
    "2. **Use `env={\"HF_MODEL_ID\": ...}`** instead of S3 paths for learning\n",
    "3. **Always clean up endpoints** after testing\n",
    "4. **Serverless = cheap but slow** / **Real-time = fast but expensive**\n",
    "5. **GPU for inference** only for large models or high throughput"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c61908e",
   "metadata": {
    "hidden": true
   },
   "source": [
    "---\n",
    "\n",
    "## ‚úÖ What We Accomplished\n",
    "\n",
    "| Step | Status |\n",
    "|------|--------|\n",
    "| Created IAM Role (`SageMakerExecutionRole`) | ‚úÖ |\n",
    "| Installed SageMaker SDK (v2) | ‚úÖ |\n",
    "| Deployed serverless HuggingFace model | ‚úÖ |\n",
    "| Tested sentiment analysis | ‚úÖ |\n",
    "| Cleaned up all resources | ‚úÖ |\n",
    "| **Total cost** | **$0.00** üéâ |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20dce323",
   "metadata": {
    "time_run": "2025-12-23T13:38:53.858249+00:00"
   },
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "from dialoghelper import curr_dialog\n",
    "\n",
    "def deploy_notebook():\n",
    "    nb_name = Path(curr_dialog()['name']).name + '.ipynb'\n",
    "    src = f'/app/data/{curr_dialog()[\"name\"]}.ipynb'\n",
    "    dst = '/app/data/publish/portfolio/static/'\n",
    "    print(nb_name)\n",
    "    \n",
    "    # Copy notebook to static folder\n",
    "    subprocess.run(['cp', src, dst])\n",
    "    \n",
    "    # Deploy with plash\n",
    "    subprocess.run(['plash_deploy'], cwd='/app/data/publish/portfolio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee84e2f",
   "metadata": {
    "time_run": "2025-12-23T13:39:19.462866+00:00"
   },
   "outputs": [],
   "source": [
    "#| hide\n",
    "deploy_notebook()"
   ]
  }
 ],
 "metadata": {
  "solveit_dialog_mode": "learning",
  "solveit_ver": 2
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
